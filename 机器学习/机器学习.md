# 分类和回归区别联系

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\监督学习分类.jpg" style="zoom:50%;" />

## 回归

从一组数据出发，确定某些变量之间的定量关系式；即建立数学模型并估计未知参数。

回归的目的是预测数值型的目标值，它的目标是接受连续数据，寻找最适合数据的方程，并能够对特定值进行预测。这个方程称为回归方程，而求回归方程显然就是求该方程的回归系数，求这些回归系数的过程就是回归。

## 线性回归

如果 2 个或者多个变量之间存在“线性关系”，那么我们就可以通过历史数据，摸清变量之间的“套路”，建立一个有效的模型，来预测未来的变量结果。

### 一元线性回归

我们学过的一元一次方程，x作为自变量，y作为因变量，就是一个连续型的回归，例如下图的年龄和收入的关系

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\一元线性回归.jpeg" style="zoom:50%;" />

### 多元线性回归

回归可能是多元的，有很多不同的变量，如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析，公式如下：

hat{y}(w, x) = w0 + w1*x1 + ... + wp*xp

那么这个公式中x前面有很多系数，这些系数是怎么求得的呢？就是通过线性回归最常用的方法：最小二乘法，这个方法简单了说就是我们通过使拟合得到的预测数据与实际数据之间的误差的平方和是最小的而得出来得式子（这个在下一部分也会讲到）

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\线性回归求法步骤1.jpg" style="zoom:50%;" />

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\线性回归求法步骤二.jpg" style="zoom:50%;" />

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\线性回归求法步骤三.jpg" style="zoom:50%;" />

#### 最小二乘法求解过程

![](C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\最小二乘法求解过程.png)

### 一元回归拟合的结果

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\回归拟合.png" style="zoom:50%;" />

几乎任意数据集都可以用上述方法建立模型，怎么判断这些模型的好坏呢？为了计算预测值序列和真实值序列的匹配程度，可以计算出这两个序列的相关系数。

![](C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\相关系数.png)

公式就是用X、Y的协方差除以X的标准差和Y的标准差。协方差是衡量两个变量变化趋势是否相似的一种方法，是同向变化(同时变大或变小)还是反向变化(一个变大一个变小), 同向或者反向的程度如何，计算公式如下:

![](C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\协方差.png)

从上面的结果图可以看出，线性回归的一个问题是可能会欠拟合，标准的线性回归是一种最小均方差的无偏差估计，在计算所有点的时候都是无偏差的计算误差，如果针对不同的点能够对误差进行调整便可以一定程度上避免标准线性回归带来的欠拟合现象。接下来介绍的局部加权线性回归就是采取这种方法对值进行预测。

### **局部加权线性回归**

在该算法中，我们给带预测点附近的每个点赋予一定的权重，越靠近预测点的数据点分配的权重越高，于分类算法kNN一样，此算法每次预测均需事先选取出对应的数据子集，算法代价高。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\拟合很好.png" style="zoom:50%;" />

### 线性回归的优缺点

优点：建模速度快，不需要很复杂的计算，在数据量大的情况下依然运行速度很快。可以根据系数给出每个变量的理解和解释

缺点：不能很好地拟合非线性数据。所以需要先判断变量之间是否是线性关系。

### 为什么在深度学习大杀四方的今天还使用线性回归？

一方面，线性回归所能够模拟的关系其实远不止线性关系。线性回归中的“线性”指的是系数的线性，而通过对特征的非线性变换，以及广义线性模型的推广，输出和特征之间的函数关系可以是高度非线性的。另一方面，也是更为重要的一点，线性模型的易解释性使得它在物理学、经济学、商学等领域中占据了难以取代的地位。

## 分类

### 分类和回归的区别

定量输出称为回归，或者说是连续变量预测；

定性输出称为分类，或者说是离散变量预测。

分类也是一种回归。分类模型的目的也是为了探寻自变量和因变量之间的相互关系，只不过在分类模型中因变量是“离散的”

特殊：逻辑回归是为了分类

举个例子：

预测明天的气温是多少度，这是一个回归任务；

预测明天是阴、晴还是雨，就是一个分类任务。

## 常见回归模型

### **Logit**

被解释变量取值0或1

二元 Logit 模型

多元逻辑回归模型

https://zhuanlan.zhihu.com/p/27188729

### logistic

#### logit和logistic模型的区别

http://www.360doc.com/content/20/0620/17/70573487_919574891.shtml

### Probit

被解释变量取值0或1

Probit模型也是一种广义的线性模型，当因变量为分类变量时，有四种常用的分析模型：

- 线性概率模型
- Logistic
- Probit
- 对数线性模型

和Logistic回归一样，Probit回归也分为：二分类Probit回归、有序多分类Probit回归、无序多分类Probit回归

Tobit

被解释变量取值（0,1）

### OSL多元线性回归

被解释变量（-，+）

# 1.分类问题

## 1.1 （主成分分析）PCA

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\PCA.jpg" style="zoom:50%;" />

把原始的数据投影到最大第二大等特征向量上

https://zhuanlan.zhihu.com/p/77151308

## 1.2 线性判别分析（LDA）

![](C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\LDA.png)

思想：降维

用作两分类,有时候也可以做多分类

### 1.2.1LDA和PCA区别

PCA为非监督降维，LDA为有监督降维

PCA希望投影后的数据方差尽可能的大（最大可分性），因为其假设方差越多，则所包含的信息越多；而LDA则希望投影后相同类别的组内方差小，而组间方差大。LDA能合理运用标签信息，使得投影后的维度具有判别性，不同类别的数据尽可能的分开。

相同点：
两者都可对数据完成降维操作
两者在降维时都使用了矩阵分解的思想
两者都假设数据服从高斯分布（正态分布）
不同点：
LDA是监督学习；PCA是无监督学习
LDA除了降维外，还可以用于分类
LDA使得类内距离小，类间距离大；PCA去除原始数据中冗余的特征，使得投影在各个维度的方差尽可能大
LDA关注分类能力，不保证投影到的坐标系是正交的；PCA投影的坐标系都是正交的

## 1.3 贝叶斯和决策树

### 1.3.1 贝叶斯公式

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\贝叶斯公式.png" style="zoom:50%;" />

贝叶斯做分类器强调的是后验概率

真正做的时候用的是朴素贝叶斯公式

## 1.4 决策树

### 1.4.1 定义

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\决策树.png" style="zoom:50%;" />

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\决策树2.png" style="zoom:50%;" />

### 1.4.2 找到最短的决策树

a.怎样选择属性，可分性

b.剪枝

### 1.4.3 过学习

两个分类器A和B，训练集误差A<B，测试集误差A>B  所以A过学习

## 1.5 支持向量机（SVM）

向高维空间做个映射

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\SVM.png" style="zoom:50%;" />

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\SVM1.png" style="zoom:50%;" />

### 1.5.1 SVM解决线性不可分问题

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\SVM2.png" style="zoom:50%;" />

#### 1.5.2 发展过程

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\SVM发展历史.png" style="zoom:50%;" />

# 2.聚类问题

## 2.1 K-Means

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\K-means.png" style="zoom:50%;" />

## 2.2密度

## 2.3 关联规则和支持度

# 3.神经网络

神经元又叫感知机。深度学习就是多层感知机

## 3.1 感知机

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\感知机.png" style="zoom:50%;" />

感知机对应一个超平面

## 3.2 感知机实现与或

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\感知机实现与或.png" style="zoom:50%;" />

## 3.3 会学习的神经元

就是不断地训练调整权重参数

## 3.4 感知机不能解决线性不可分问题

感知机就是一个线性分类器，不能解决线性不可分问题

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\线性不可分问题.png" style="zoom:50%;" />

## 3.5 多层感知机（神经网络）

![]()<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\神经网络.png" alt="神经网络" style="zoom:50%;" />

### 3.5.1 解决线性不可分

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\神经网络线性不可分.png" style="zoom:50%;" />

映射思想，隐含层表示，向隐含层做一个映射

隐含层变化

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\隐含层变化.png" style="zoom:50%;" />

神经元和感知机类似但也有区别，但也有区别，激活函数不一样。感知机是门限的激活函数、神经元是sigmoid Function 

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\感知机.png" style="zoom:50%;" />

### 3.5.2 训练学习和逆传播

都是要求误差对某一个权重求偏导

**输出层**

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\训练神经网络.png" style="zoom:50%;" />

**隐含层**

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\隐含层训练.png" style="zoom:50%;" />

## 3.6 神经网络的用途

目前深度学习中的神经网络种类繁多，用途各异。分类、回归、聚类等都有用到。

由于这个分支在指数增长，跟踪神经网络的不同拓扑有助于更深刻的理解。神经网络中最常用的拓扑结构总结如下：https://mbd.baidu.com/ma/s/LgLiPUWN

# 4.集成学习

不是特定算法，而是一个大的算法框架

有策略的生成很多分类器，然后有策略的组合在一起

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\集成学习.png" style="zoom:50%;" />

## 4.1 Bagging 和 Boosting

思想：分类器一定要不一样，不同指1.不同的分类器，例如SVM、决策树、神经网络2.同一个分类器训练集不同或初始值不同或特征不同

### 4.1.1 Bagging

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\Bagging.png" style="zoom:50%;" />

每个分类器的输出没有权重

**stacking**

学习输出权重

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\stacking.png" style="zoom:50%;" />

### 4.1.2 Boosting

第二个分类器专门训练第一个分类器分错的样本

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\boosting.png" style="zoom:50%;" />

专门解决争端的

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\boosting解决争端.png" style="zoom:50%;" />

adaboost

![](C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\adaboost.png)

Dynamic Weight schema

动态权重，每个分类器的发言权不断变化的

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\Dynamic_Weight.png" style="zoom:50%;" />

# 5.机器学习整体框架

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\机器学习\image\机器学习整体框架.png" style="zoom:50%;" />