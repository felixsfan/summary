# 1. 分布式理论

## 1.1 模型

### **1.1.1 节点**

在具体的工程项目中，一个节点往往是一个操作系统上的进程。在本文的模型中，认为节点是一个完整的、不可分的整体，如果某个程序进程实际上由若干相对独立部分构成，则在模型中可以将一个进程划分为多个节点。

### **1.1.2 异常**

1. 机器宕机：机器宕机是最常见的异常之一。在大型集群中每日宕机发生的概率为千分之一左右，在实践中，一台宕机的机器恢复的时间通常认为是24 小时，一般需要人工介入重启机器。

2. 非拜占庭错误

   网络异常：

   消息丢失：两片节点之间彼此完全无法通信，即出现了“网络分化”；

   消息乱序：有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；

   数据错误：不可靠的TCP，TCP 协议为应用层提供了可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于TCP 协议则通信就是可靠的。（属于拜占庭错误）

   消息延时：由于网络拥挤，网络抖动造成消息传递的延时

3. 拜占庭错误

   恶意响应，伪造身份

4. 分布式三态：如果某个节点向另一个节点发起RPC(Remote procedure call)调用，即某个节点A 向另一个节点B 发送一个消息，节点B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点A，那么这个RPC 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称之为分布式系统的三态。

5. 存储数据丢失:对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。

6. 异常处理原则：被大量工程实践所检验过的异常处理黄金原则是：任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

### 1.1.3节点管理

服务调用失败一般是由两类原因引起的，一类是服务提供者自身出现问题，如服务器宕机、进程意外退出等；一类是网络问题，如服务提供者、注册中心、服务消费者这三者任意两者之间的网络出现问题。无论是服务提供者自身出现问题还是网络发生问题，都有两种节点管理手段。

#### 1.1.3.1 注册中心主动摘除机制

这种机制要求服务提供者定时的主动向注册中心汇报心跳，注册中心根据服务提供者节点最近一次汇报心跳的时间与上一次汇报心跳时间做比较，如果超出一定时间，就认为服务提供者出现问题，继而把节点从服务列表中摘除，并把最近的可用服务节点列表推送给服务消费者。

或者注册中心主动发送心跳包探活

#### 1.1.3.2 服务消费者摘除机制

虽然注册中心主动摘除机制可以解决服务提供者节点异常的问题，但如果是因为注册中心与服务提供者之间的网络出现异常，最坏的情况是注册中心会把服务节点全部摘除，导致服务消费者没有可用的服务节点调用，但其实这时候服务提供者本身是正常的。所以，将存活探测机制用在服务消费者这一端更合理，如果服务消费者调用服务提供者节点失败，就将这个节点从内存中保存的可用服务提供者节点列表中移除。

## 1.2 副本

### 1.2.1 概念

副本（replica/copy）指在分布式系统中为数据或服务提供的冗余。对于数据副本指在不同的节点上持久化同一份数据，当出现某一个节点的存储的数据丢失时，可以从副本上读到数据。

**数据副本是分布式系统解决数据丢失异常的唯一手段。**另一类副本是**服务副本**，指数个节点提供某种相同的服务，这种服务一般并不依赖于节点的本地存储，其所需数据一般来自其他节点。

副本协议是贯穿整个分布式系统的理论核心。

### **1.2.2 副本一致性**

> 和CAP理论中的一致性一样

分布式系统通过副本控制协议，使得从系统外部读取系统内部各个副本的数据在一定的约束条件下相同，称之为副本一致性(consistency)。**副本一致性是针对分布式系统而言的，不是针对某一个副本而言。**

1. 强一致性(strong consistency)：任何时刻任何用户或节点都可以读到最近一次成功更新的副本数据。强一致性是程度最高的一致性要求，也是实践中最难以实现的一致性。

2. 单调一致性(monotonic consistency)：任何时刻，任何用户一旦读到某个数据在某次更新后的值，这个用户不会再读到比这个值更旧的值。

   单调一致性是弱于强一致性却非常实用的一种一致性级别。因为通常来说，用户只关心从己方视角观察到的一致性，而不会关注其他用户的一致性情况。

3. 会话一致性(session consistency)：任何用户在某一次会话内一旦读到某个数据在某次更新后的值，这个用户在这次会话过程中不会再读到比这个值更旧的值。

   会话一致性通过引入会话的概念，在单调一致性的基础上进一步放松约束，会话一致性只保证单个用户单次会话内数据的单调修改，对于不同用户间的一致性和同一用户不同会话间的一致性没有保障。

   实践中有许多机制正好对应会话的概念，例如php 中的session 概念。

4. 最终一致性(eventual consistency)：最终一致性要求一旦更新成功，各个副本上的数据最终将达 到完全一致的状态，但达到完全一致状态所需要的时间不能保障。

   对于最终一致性系统而言，一个用户只要始终读取某一个副本的数据，则可以实现类似单调一致性的效果，但一旦用户更换读取的副本，则无法保障任何一致性。

5. 弱一致性(week consistency)：一旦某个更新成功，用户无法在一个确定时间内读到这次更新的值，且即使在某个副本上读到了新的值，也不能保证在其他副本上可以读到新的值。

   弱一致性系统一般很难在实际中使用，使用弱一致性系统需要应用方做更多的工作从而使得系统可用。

### 1.2.3 副本更新策略

和副本协议、一致性算法一样，侧重点在冗余存储方面

#### 1.2.3.1 同时更新

- 类型A：没有任何协议，可能出现多个节点执行顺序交叉导致数据不一致情况。
- 类型B：通过一致性协议唯一确定不同更新操作的执行顺序，从而保证数据一致性

#### 1.2.3.2 主从式更新

多个副本之间存在一个主副本（Master Replica），其他副本为从副本，这种称为主从更新策略。所有对数据的更新首先提交到主副本，再由主副本通知从副本进行数据更新。如果同时产生多个数据更新操作，由主副本决定不同更新操作的顺序。

> 在MySQL上的应用就是MySQL主从复制

##### 类型A：同步方式

主副本等待所有从副本更新完成之后才确认更新操作完成，这样确保数据的强一致性，但是会存在较大的请求延时，尤其是在多副本跨数据中心的情形下，因为请求延时取决于最慢的那个副本的更新速度。

##### 类型B：异步方式

主副本在通知从副本更新之前即可确认更新操作。假设主副本还没有通知任何其他从副本就发生崩溃，那么数据一致性可能会出现问题，一般首先在另外的可靠存储位置将这次更新操作记录下来，以防这种情况发生。

- 1）所有读请求都通过主副本来响应，任意一个副本接收到读请求后转发为主副本，可以保证强一致，但是本来可以由距离近的副本响应的操作又得转发给距离较远的主副本，增加了请求延时，**Google的Chubby采用这种方式**。
- 2）任意一个副本都可以响应读请求，请求延时大大降低，但是可能导致读不一致，因为有些副本可能还存在旧版本的数据，**Zookeeper就是采用这种方法获得低延时，但牺牲了一致性**。

##### 类型C：半同步方式

同步混合异步，主副本首先同步更新部分从副本，然后确认更新操作完成，其他副本通关异步方式获得更新，**Kafka就是采用这种混合方式来维护数据副本的不一致性**。

- 1）读操作至少要从一个同步更新的节点读出，类似RWN协议的R+W>N，可保证强一致性，但是请求延时加大
- 2）读操作不要求一定从至少一个同步更新节点读出，那么会出现类型B的第2种不一致性情形。

#### 1.2.3.3 任意节点更新

不区分主从副本，任意节点都可以接收请求，然后又它去通知其他副本进行更新。

##### 类型A：同步通知其他副本

存在和主从更新的类型A的情况，除此之外，为了识别出是否存在不同客户端向不同副本发送对同一数据的更新操作，还需要额外付出更多的请求延时

##### 类型B：异步通知其他副本

存在主从更新的B方式问题。

**Dynamo/Cassandra/Riak同时采取了主从式更新的类型C（同步+异步），以及任意节点更新的策略**。

## **1.3 数据分布方式**

**参考mysql向外扩展**

所谓分布式系统顾名思义就是利用多台计算机协同解决单台计算机所不能解决的计算、存储等问题。

单机系统与分布式系统的最大的区别在于问题的规模，即计算、存储的数据量的区别。

将一个单机问题使用分布式解决，首先要解决的就是如何将问题拆解为可以使用多机分布式解决，使得分布式系统中的每台机器负责原问题的一个子集。由于无论是计算还是存储，其问题输入对象都是数据，所以如何拆解分布式系统的输入数据成为分布式系统的基本问题。

### 1.3.1 哈希方式

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\数据分布哈希方式.png" style="zoom:50%;" />

哈希分布数据的缺点同样明显，突出表现为可扩展性不高，一旦集群规模需要扩展，则几乎所有的数据需要被迁移并重新分布。工程中，扩展哈希分布数据的系统时，往往使得集群规模成倍扩展，按照数据重新计算哈希，这样原本一台机器上的数据只需迁移一半到另一台对应的机器上即可完成扩展。

针对哈希方式扩展性差的问题，一种思路是不再简单的将哈希值与机器做除法取模映射，而是将对应关系作为元数据由专门的元数据服务器管理.同时，哈希值取模个数往往大于机器个数，这样同一台机器上需要负责多个哈希取模的余数。但需要以较复杂的机制维护大量的元数据。哈希分布数据的另一个缺点是，一旦某数据特征值的数据严重不均，容易出现“数据倾斜”（data skew）问题。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\哈希取模的不足.png" style="zoom:50%;" />

### 1.3.2 按数据范围分布

按数据范围分布是另一个常见的数据分布式，将数据按特征值的值域范围划分为不同的区间，使得集群中每台（组）服务器处理不同区间的数据。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\按数据范围分布.png" style="zoom:50%;" />

工程中，为了数据迁移等负载均衡操作的方便，往往利用动态划分区间的技术，使得每个区间中服务的数据量尽量的一样多。当某个区间的数据量较大时，通过将区间“分裂”的方式拆分为两个区间，使得每个数据区间中的数据量都尽量维持在一个较为固定的阈值之下。

一般的，往往需要使用专门的服务器在内存中维护数据分布信息，称这种数据的分布信息为一种元信息。甚至对于大规模的集群，由于元信息的规模非常庞大，单台 计算机无法独立维护，需要使用多台机器作为元信息服务器。

### 1.3.3 按数据量分布

与按数据范围分布数据的方式类似的是，按数据量分布数据也需要记录数据块的具体分布情况，并将该分布信息作为元数据使用元数据服务器管理。

由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。

当集群需要重新负载均衡时，只需通过迁移数据块即可完成。集群扩容也没有太大的限制，只需将部分数据库迁移到新加入的机器上即可以完成扩容。

按数据量划分数据的缺点是需要管理较为复杂的元信息，与按范围分布数据的方式类似，当集群规模较大时，元信息的数据量也变得很大，高效的管理元信息成为新的课题。

### 1.3.4 一致性哈希

一致性哈希（consistent hashing）是另一个种在工程中使用较为广泛的数据分布方式。一致性哈希最初在P2P 网络中作为分布式哈希表（DHT）的常用数据分布算法。

一致性哈希的基本方式是使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，即哈希函数输出的最大值是最小值的前序。将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\一致性哈希.png" style="zoom:50%;" />

使用一致性哈希的方式需要将节点在一致性哈希环上的位置作为元信息加以管理，这点比直接使用哈希分布数据的方式要复杂。然而，节点的位置信息只于集群中的机器规模相关，其元信息的量通常比按数据范围分布数据和按数据量分布数据的元信息量要小很多。

为此一种常见的改进算法是引入虚节点（virtual node）的概念，系统初始时就创建许多虚节点，虚节点的个数一般远大于未来集群中机器的个数，将虚节点均匀分布到一致性哈希值域环上，其功能与基本一致性哈希算法中的节点相同。为每个节点分配若干虚节点。

操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点。使用虚节点改进有多个优点。

首先，一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点负载失效节点的压力。同理，一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以 负载多个原有节点的压力，从全局看，较容易实现扩容时的负载均衡。

### 1.3.5 副本与数据分布

分布式系统容错、提高可用性的基本手段就是使用副本。对于数据副本的分布方式主要影响系统的可扩展性。一种基本的数据副本策略是以机器为单位，若干机器互为副本，副本机器之间的数据完全相同。这种策略适用于上述各种数据分布方式。其优点是非常简单，其缺点是恢复数据的效率不高、可扩展性也不高。

更合适的做法不是以机器作为副本单位，而是将数据拆为较合理的数据段，以数据段为单位作为副本。

实践中，常常使得每个数据段的大小尽量相等且控制在一定的大小以内。数据段有很多不同的称谓，segment，fragment，chunk，partition 等等。数据段的选择与数据分布方式直接相关。

对于哈希分数据的方式，每个哈希分桶后的余数可以作为一个数据段，为了控制数据段的大小，常常使得分桶个数大于集群规模。一旦将数据分为数据段，则可以以数据段为单位管理副本，从而副本与机器不再硬相关，每台机器都可以负责一定数据段的副本。

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\副本与数据分布.png)

一旦副本分布与机器无关，数据丢失后的恢复效率将非常高。这是因为，一旦某台机器的数据丢失，其上数据段的副本将分布在整个集群的所有机器中，而不是仅在几个副本机器中，从而可以从整个集群同时拷贝恢复数据，而集群中每台数据源机器都可以以非常低的资源做拷贝。作为恢复数据源的机器即使都限速1MB/s，若有100 台机器参与恢复，恢复速度也能达到100MB/s。

再者，副本分布与机器无关也利于集群容错。如果出现机器宕机，由于宕机机器上的副本分散于整个集群，其压力也自然分散到整个集群。

最后，副本分布与机器无关也利于集群扩展。理论上，设集群规模 为N 台机器，当加入一台新的机器时，只需从各台机器上迁移1/N – 1/N+1 比例的数据段到新机器即实现了新的负载均衡。由于是从集群中各机器迁移数据，与数据恢复同理，效率也较高。

工程中，完全按照数据段建立副本会引起需要管理的元数据的开销增大，副本维护的难度也相应增大。一种折中的做法是将某些数据段组成一个数据段分组，按数据段分组为粒度进行副本管理。这样做可以将副本粒度控制在一个较为合适的范围内。

### 1.3.6 本地化计算

在分布式系统中，数据的分布方式也深深影响着计算的分布方式。在分布式系统中计算节点和保存计算数据的存储节点可以在同一台物理机器上，也可以位于不同的物理机器。

如果计算节点和存储节点位于不同的物理机器则计算的数据需要通过网络传输，此种方式的开销很大，甚至网络带宽会成为系统的总体瓶颈。

另一种思路是，将计算尽量调度到与存储节点在同一台物理机器上的计算节点上进行，这称之为本地化计算。本地化计算是计算调度的一种重要优化，其体现了一种重要的分布式调度思想：“移动数据不如移动计算”。

## **1.4 副本协议**

> **和分布式一致性算法大同小异**

对于数据存储而言，**为了提高可用性（Availability），采用了副本备份**，比如对于HDFS，默认每块数据存三份。某数据块所在的机器宕机了，就去该数据块副本所在的机器上读取（从这可以看出，数据分布方式是按“数据块”为单位分布的）

但是，问题来了，当需要修改数据时，就需要更新**所有**的副本数据，这样才能保证数据的一致性。

**副本控制协议指按特定的协议流程控制副本数据的读写行为，使得副本满足一定的可用性和一致性要求的分布式协议。**副本控制协议要具有一定的对抗异常状态的容错能力，从而使得系统具有一定的可用性，同时副本控制协议要能提供一定一致性级别。由CAP 原理可知，要设计一种满足强一致性，且在出现任何网络异常时都可用的副本协议是不可能的。为此，实际中的副本控制协议总是在可用性、一致性与性能等各要素之间按照具体需求折中。

副本控制协议可以分为两大类：“中心化(centralized)副本控制协议”和“去中心化(decentralized)副本控制协议”。

### 1.4.1 中心化副本控制协议

#### 1.4.1.1 简介

**参考mysql主从复制类似**

中心化副本控制协议的基本思路是由一个中心节点协调副本数据的更新、维护副本之间的一致性。

图给出了中心化副本协议的通用架构。中心化副本控制协议的优点是协议相对较为简单，所有的副本相关的控制交由中心节点完成。并发控制将由中心节点完成，从而使得一个分布式并发控制问题，简化为一个单机并发控制问题。

所谓并发控制，即多个节点同时需要修改副本数据时，需要解决“写写”、“读写”等并发冲突。单机系统上常用加锁等方式进行并发控制。对于分布式并发控制，加锁也是一个常用的方法，但如果没有中心节点统一进行锁管理，就需要完全分布式化的锁系统，会使得协议非常复杂。

中心化副本控制协议的缺点是系统的可用性依赖于中心化节点，当中心节点异常或与中心节点通信中断时，系统将失去某些服务（通常至少失去更新服务），所以中心化副本控制协议的缺点正是存在一定的停服务时间。

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\中心化副本协议.png)

*中心化副本控制协议*主要有primary-secondary协议、2PC、MVCC

#### 1.4.1.2 primary-secondary 协议

在primary-secondary 类型的协议中，副本被分为两大类，其中有且仅有一个副本作为primary 副本，除primary 以外的副本都作为secondary 副本。维护primary 副本的节点作为中心节点，中心节点负责维护数据的更新、并发控制、协调副本的一致性。

Primary-secondary 类型的协议一般要解决四大类问题：

数据更新流程、数据读取方式、Primary 副本的确定和切换、数据同步（reconcile）。

##### a.数据更新基本流程

1. 数据更新都由primary 节点协调完成。
2. 外部节点将更新操作发给primary 节点
3. primary 节点进行并发控制即确定并发更新操作的先后顺序
4. primary 节点将更新操作发送给secondary 节点
5. primary 根据secondary 节点的完成情况决定更新是否成功并将结果返回外部节点

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\primary-secondary类型的协议.png)

在工程实践中，如果由primary 直接同时发送给其他N 个副本发送数据，则每个 secondary 的更新吞吐受限于primary 总的出口网络带宽，最大为primary 网络出口带宽的1/N。

为了解决这个问题，有些系统（例如，GFS），使用接力的方式同步数据，即primary 将更新发送给第一 个secondary 副本，第一个secondary 副本发送给第二secondary 副本，依次类推。

##### b.数据读取方式

 如果只需要最终一致性，则读取任何副本都可以满足需求。如果需要会话一致性，则可以为副本设置版本号，每次更新后递增版本号，则用户读取副本时验证版本号，从而保证用户读到的数据都在会话范围内递增。想要最终一致性比较困难，由一下几种思路参考:

- 只读primary节点。
- 由primary控制节点secondary的可用性，更新成功为可用，更新不成功为不可用。
- 基于Quorum机制。

##### c. primary 副本的确定与切换

通常的，在primary-secondary 类型的分布式系统中，哪个副本是primary 这一信息都属于元信息，由专门的元数据服务器维护。执行更新操作时，首先查询元数据服务器获取副本的primary 信息，从而进一步执行数据更新流程。

切换副本的难度在于两个方面：

1. 如何确定节点的状态以及发现原primary节点异常是一个较为复杂的问题。

   **基于Lease机制(心跳机制)、基于Quorum机制（过半机制）**

2. 在原primary以及死机，如何确定一个secondary副本使得该副本上面的数据与原primary一致成为新的问题

   由于分布式系统中可靠的发现节点异常需要一点的探测时间的，探测时间通常在10秒级别。 primary-secondary类型的分布式系统的最大缺点就是primary切换带来的一定时间的停服务时间。

##### d.数据同步

不一致的secondary 副本需要与primary 进行同步（reconcile）。

通常不一致的形式有三种：

1. 由于网络分化等异常，secondary 上的数据落后于primary 上的数据。
2. 在某些协议下，secondary 上的数据有可能是脏数据，需要被丢弃。所谓脏数据是由于primary 副本没有进行某一更新操作，而secondary 副本上反而进行的多余的修改操作，从而造成secondary 副本数据错误。
3. secondary 是一个新增加的副本，完全没有数据，需要从其他副本上拷贝数据。

 对于（1），常用的方法是回放primary上面的操作日志（通常是redo日志），从而追上primary的更新进度。
 对于（2），较好的方法就是在设计分布式协议不产生脏数据。也可以基于undo日志的方法删除脏数据。
 对于（3），直接拷贝primary数据，或者使用快照。

### 1.4.2 去中心化副本控制协议

去中心化副本控制协议没有中心节点，协议中所有的节点都是完全对等的，节点之间通过平等协商达到一致。从而去中心化协议没有因为中心化节点异常而带来的停服务等问题。

去中心化协议的最大的缺点是协议过程通常比较复杂。尤其当去中心化协议需要实现强一致性时，协议流程变得复杂且不容易理解。由于流程的复杂，去中心化协议的效率或者性能一般也较中心化协议低。

**去中心化副本控制协议主要有：Paxos**

## 1.5 租约、心跳包

### 1.5.1 Lease 机制

#### 1.5.1.1 租约机制的应用

- **进行故障检测**

  类似于ZooKeeper中master 与 slaver 之间发送的心跳包的作用。在ZK中， master 和 slaver 之间通过交换心跳包来检测它们是否还存活。

  **在通常的集群系统中，我们采用心跳来检测节点状态。但普通的心跳机制是无协议和承诺约定的，所以它的检测结果可能不可靠。很多监控系统采用心跳检测集群中节点的存活性，这种机制存在误报警的可能。**

  普通心跳通常是在规定的时限内定期向检测节点发送存活性报告，若超出一段时间未能收到心跳报告，那么检测节点则判断节点可能失效，并采取一系列措施（报警、通知节点的使用者）。这种机制存在的问题是，检测节点单方面判定节点失效，在某些业务集群系统中可能存在风险。节点自身并未认识自己已被认定失效，还在继续提供正常的服务。若该节点在集群中承担唯一 primary 节点的职责，而检测节点的失效判定发起了重新选择新的主节点，会引发“双主”问题。

  ​    **采用 lease 机制的心跳实现，则彻底避免了此类问题。**由于网络分割的原因，其实没有任何技术可以可靠的判定节点状态，但采用 lease 机制的状态检测，可以避免出现误判时引入新的问题。

- **维护缓存一致性**

  1. 每次读取数据时都先询问服务器数据是不是最新的，若不是，则先让服务器传输新数据，然后再读取该新数据。

  2. 回调，由服务器记录有哪些客户端读取了数据，当服务器对数据做修改时先通知记录下来的这些客户端，上次读取过的数据已经失效。这二种方法都有一定的缺陷。
  3. **租约机制**

  1989年斯坦福大学的Cary G. Gray和David R. Cheriton提出了利用 **租约**来维护缓存一致性的方法。所谓租约，其实就是一个合同，即服务器给予客户端在 **一定期限**内可以 **控制修改操作**的 权力。如果服务器要修改数据，首先要征求拥有这块数据的租约的客户端的同意，之后才可以修改。客户端从服务器读取数据时往往就同时获取租约，在租约期限内，如果没有收到服务器的修改请求，就可以保证当前缓存中的内容就是最新的。如果在租约期限内收到了修改数据的请求并且同意了，就需要清空缓存。在租约过期以后，客户端如果还要从缓存读取数据，就必须重新获取租约，我们称这个操作为“ **续约**”。

  在租约期限内，客户端可以保证其缓存中的数据是最新的。同时，租约可以容忍各种**非拜占庭式失效**（机器崩溃、网络分割等）。如果客户端崩溃或者网络中断，服务器只需要等待其租约过期就可以进行修改操作。如果服务器出错丢失了所有客户端的信息，它只需要知道租约的最长期限，就可以在这个期限之后安全的修改数据。

- **分布式文件系统**

  以hadoop的hdfs为例，每个文件块都有多个副本分布在多个slave上，在 并行追加时必须有一个全局统一的追加顺序。当然这个顺序可以由master来确定，但是这样会大大增加master的负荷。另一种方法可以由多个 slave通过一致性协议（比如Paxos）来达成一个一致，但这样开销太大。hdfs使用了租约机制，就是对每个文件块，由master向一 个slave发放租约，在租约期限内就由它来负责并行追加操作的顺序。slave正常运行时可以一直续约，如果出现了机器失效或 者网络分割的情况，master就在租约过期以后把租约交给另外一个slave。在某些情况下，master也会联系拥有租约的 slave，请它们提前释放租约。

#### 1.5.1.2 租约总结

在很多时候，租约的定义似乎很模糊，有的时候租约类似心跳，有的时候又类似于锁。到底租约的本质是什么呢？
   回到租约最原始的定义：租约就是在一定期限内给予持有者特定权力的协议。我觉得这里的期限就是租约的根本特性，正是这一特性使得租约可以容忍机器失效和网络分割。在期限之内，租约其实就是服务器和客户端之间的协议，而这个协议的内容可以五花八门。

  1：如果协议内容是服务器确认客户端还存活，那么这个租约的功能就相当于**心跳**；

  2：如果协议内容是服务器保证内容不会被修改，那么这个 租约就相当于**读锁**；

  3：如果协议内容是服务器保证内容只能被这个客户端修改，那么这个租约就相当于**写锁**。

  租约这种灵活性和容错性，使其成为了维护分布式系统一致性的有效工具。

## **1.6 Quorum 机制**

待完善

## **1.7 日志技术**

日志技术是宕机恢复的主要技术之一。日志技术最初使用在数据库系统中。严格来说日志技术不是一种分布式系统的技术，但在分布式系统的实践中，却广泛使用了日志技术做宕机恢复，甚 至如BigTable 等系统将日志保存到一个分布式系统中进一步增强了系统容错能力。

### Redo Log 与Check point

参考mysql文件夹

## **1.8 衡量分布式系统的指标**

### 1.8.1 性能

#### 1.8.1.1 性能指标

##### **系统的吞吐能力**

指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；

##### **系统的响应延迟**

指系统完成某一功能需要使用的时间；

##### **系统的并发能力**

指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。

上述三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS。

#### 1.8.1.2 量化指标

##### 网络带宽

##### QPS

##### TPS

##### 连接数

##### 数据量级

### 1.8.2 可用性

系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。

系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。

### 1.8.3 可扩展性

系统的可扩展性(scalability)指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。

### 1.8.4 一致性

分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致的性模型，对于用户使用来说使用起来越简单。

#### 1.8.4.1 强一致性

任何时刻任何用户或节点都可以读到最近一次成功更新的副本数据。

#### 1.8.4.2 弱一致性

一旦某个更新成功，用户无法在一个确定时间内读到这次更新的值，且即使在某个副本上读到了新的值，也不能保证在其他副本上可以读到新的值。

#### 1.8.4.3 最终一致性

最终一致性要求一旦更新成功，各个副本上的数据最终将达 到完全一致的状态，但达到完全一致状态所需要的时间不能保障。

## **1.9 四大基础理论**

1. 拜占庭将军问题
2. CAP 理论
3. ACID 理论
4. BASE 理论

### 1.9.1 CAP理论

CAP理论为：一个分布式系统最多只能同时满足**一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）**这三项中的两项

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\CAP.jpg)

#### 1.9.1.1 分区容错性（Partition tolerance）

系统应该能持续提供服务，即使系统内部有消息丢失（分区）。

大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\分区容错.png" style="zoom:50%;" />

上图中，G1 和 G2 是两台跨区的服务器。G1 向 G2 发送一条消息，G2 可能无法收到。系统设计的时候，必须考虑到这种情况。在网络中断，消息丢失的情况下，系统照样能够工作。 以实际效果而言，分区相当于对通信的时限要求。**系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择**

一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。

#### 1.9.1.2  一致性（Consistency）

写操作之后的读操作，必须返回该值。举例来说，某条记录是 v0，用户向 G1 发起一个写操作，将其改为 v1。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\一致性1.png" style="zoom:50%;" />

接下来，用户的读操作就会得到 v1。这就叫一致性

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\一致性2.png" style="zoom:50%;" />

问题是，用户有可能向 G2 发起读操作，由于 G2 的值没有发生变化，因此返回的是 v0。G1 和 G2 读操作的结果不一致，这就不满足一致性了。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\一致性3.png" style="zoom:50%;" />

为了让 G2 也能变为 v1，就要在 G1 写操作的时候，让 G1 向 G2 发送一条消息，要求 G2 也改成 v1。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\一致性4.png" style="zoom:50%;" />

这样的话，用户向 G2 发起读操作，也能得到 v1。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\一致性5.png" style="zoom:50%;" />

#### 1.9.1.3 可用性（Availability）

每一个操作总是能够在一定的时间内返回结果，这里需要注意的是"一定时间内"和"返回结果"。一定时间指的是，在可以容忍的范围内返回结果，结果可以是成功或者失败。

用户可以选择向 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。

#### 1.9.1.4 Consistency 和 Availability 的矛盾

一致性和可用性，为什么不可能同时成立？答案很简单，因为可能通信失败（即出现分区容错）。

如果保证 G2 的一致性，那么 G1 必须在写操作时，锁定 G2 的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2 不能读写，没有可用性不。

如果保证 G2 的可用性，那么势必不能锁定 G2，所以一致性不成立。

综上所述，G2 无法同时做到一致性和可用性。系统设计时只能选择一个目标。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。

### 1.9.2 BASE理论

BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。

BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）

#### 1.9.2.1 基本可用

基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。

电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。

#### 1.9.2.2 软状态

软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。mysql replication的异步复制也是一种体现。

#### 1.9.2.3 最终一致性

最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。

### 1.9.3 **拜占庭将军问题**

大家可能听过拜占庭将军问题。它是由莱斯利·兰伯特提出的点对点通信中的基本问题。

拜占庭位于如今的土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了达到防御目的，每个军队都分隔很远，**将军与将军之间只能靠信差传消息**。在战争的时候，拜占庭军队内所有将军和副官必须达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，这个就是拜占庭容错问题。

实际上拜占庭问题是**分布式领域最复杂的一个容错模型。**一旦理解它，就能掌握分布式共识问题的解决思路，还能帮助大家理解常用的共识算法，也可以帮助我们在工作中选择合适的算法，或者设计合适的算法。

为什么第一个基础理论是拜占庭将军问题？

因为它很好地**抽象出了分布式系统面临的共识问题**。上面提到的 8 种分布式算法中有 5 种跟拜占庭问题相关，可以说弄懂拜占庭问题对后面学习其他算法就会容易很多。

下面我用三国杀游戏中的身份牌来讲解拜占庭将军问题。

#### 1.9.3.1 问题描述

##### **1.9.3.1.1  三国杀身份牌**

三国杀中主要有四种身份：**主公、忠臣、反贼、内奸**。每个游戏玩家都会获得一个身份牌。主公只有 1 个。忠臣最多 2 个，反贼最多 4个，内奸最多一个。

###### **主公**

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzvQzwzO4uOtE540vfgBBhkwkCSIeqnKTqTXnTlibabFyleAv24WD7QeQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

主公身份牌

**获胜条件：消灭所有反贼和内奸**

技巧：以自己生存为首要目标，分散反贼注意力。配合忠内剿灭反贼并判断谁是忠谁是内。

###### **忠臣**

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0Zmcpgz38AxcDD5ukFyjMLVqZe3SbdBKRCW1hSA931Ljz5p1AWTwk8uOR0Fpg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

忠臣身份牌

**获胜条件：保护主公存活的前提下消灭所有反贼和内奸。**

技巧：忠臣是主公的屏障，威慑反贼和内奸的天平。

###### **反贼**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzDT19k0tOshnDoicXndDj49olUa8t2ia6YMYILYU3ygSd2c4iaGePhBkRg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

反贼身份牌

**获胜条件：消灭主公即可获胜。**

技巧：反贼作为数量最多的身份，需要集中火力猛攻敌人弱点。正确的思路是获胜的关键。

###### **内奸**

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzfjVqC7B8v6YicvSMDUVia65wY7MDwgh4JTUwueQQYpJR3Rbfg900EOLA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

内奸身份牌

**获胜条件：先消灭反贼和忠臣，最后与主公单挑成为最后唯一生还者。**

技巧：正确的战术+ 冷静的头脑+ 运气。

##### 1.9.3.1.2 还原拜占庭问题

东汉末年，袁绍作为盟主，汇合了十八路诸侯一起攻打董卓。把董卓定为反贼，袁绍定为主公，另外有两个忠臣和一个内奸，就选这三个风云人物：曹操，刘备，孙坚（孙权的爸比），内奸扮演的角色是忠臣，主公和两个忠臣不知道内奸的身份，都当作忠臣对待了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzykDiaiclWlgsWkgV3UI5icjJr4iaJIxiaw98OKjOmW2ibA96Pcz2eEiaDyicHQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

董卓是非常强大的，拥有精良的西凉兵，麾下还有战神吕布。大家都知道三英站吕布的故事，吕布以一已之力对阵刘备、张飞、关羽三人。

要想干掉董卓，**袁绍必须统一忠臣的作战计划**，三位忠臣还不知道有什么其他花花肠子，有一个还是内奸。如果内奸暗通反贼董卓，给忠臣发送误导性的作战信息，该怎么办？另外假定这几个忠臣都是通过书信交流作战信息，如果书信被拦截了或书信里面的信息被替换了咋办？这些场景都可能扰乱作战计划，最后出现有的忠臣在进攻，有的忠臣撤退了。那么反贼就可以乘此机会发起进攻，逐一攻破。

袁绍本来就没有曹操的机智，那他如何让忠臣们达成共识，制定统一的作战计划呢？

**上面的映射关系就是一个拜占庭将军问题的一个简化表述**，袁绍现在面临的就是典型的共识问题。也就是在可能有误导信息的情况下，采用合适的通讯机制，让多个将军达成共识，制定一致性的作战计划。

##### **1.9.3.1.3 一方选择撤退**

刘备、曹操、孙坚**通过信使传递进攻或撤退**的信息，然后进行协商，到底是进攻还是撤退。**遵循少数服从多数，不允许弃权**。

曹操疑心比较重，侦查了反贼的地形后，决定撤退。而刘备和孙坚决定进攻。

- 刘备决定进攻，通过信使告诉曹操和孙坚进攻。
- 曹操决定撤退，通过信使告诉刘备和孙坚撤退。
- 孙坚决定进攻，通过信使告诉曹操和刘备进攻。

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzQUNPMMnq2MOIMiaHCdD6UAjl89amtUTcT0rCZzHBZaguWa16bzt7Nsg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

一方选择撤退

曹操收到的信息：进攻 2 票，自己的一张撤退票，票数一比，进攻票：撤退票 = 2 : 1，按照上面的少数服从多数原则进行投票表决，曹操还是会进攻。**那么三方的作战方案都是进攻，所以是一个一致性的作战方案。**最后战胜了董卓。

##### **1.9.3.1.4 内奸登场-撤退**

因为我们前期的设定，孙坚作为内奸，早已与反贼董卓私下沟通好了，不攻打董卓。

- 刘备决定进攻，通过信使告诉曹操和孙坚进攻。
- 曹操决定撤退，通过信使告诉曹操和孙坚撤退。
- 孙坚决定撤退，通过信使告诉曹操和刘备撤退。

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzE3vDs7nhpAMfVcKicZ42PNMCaZcvZzicia3J3p1uafa2XbY0mSUMv8vgg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

内奸登场-撤退

刘备收到进攻和撤退各一票，而自己又选择撤退，所以刘备得到的票数是：进攻 : 撤退 = 1 : 2，遵从少数服从多数的原则，刘备选择最后选择撤退，**那么三方的作战方案都是撤退，所以也是一个一致性的作战方案。**

##### **1.9.3.1.5 内奸使诈-一进一退**

内奸看了上述计划，发现忠臣都撤退了，并没有被消灭，就想通过使诈的方式来消灭其中一个忠臣。

- 刘备决定进攻，通过信使告诉曹操和孙坚进攻。
- 曹操决定撤退，通过信使告诉曹操和孙坚撤退。
- 孙坚作为内奸使诈，通过信使告诉刘备进攻，告诉曹操撤退。

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0Zmcpgza4ByhE8KpE9II13ncGQGCHHlXt12ich8IIRwOgPcSqKHLmMpUL32eZw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

内奸使诈-一进一退

那么结果是什么呢？

刘备的票数为进攻 2 票，撤退 1 票，曹操的票数为进攻 1 票，撤退 2 票。按照少数服从多数的原则，**刘备最后会选择进攻，而曹操会选择撤退，孙坚作为内奸肯定不会进攻，刘备单独进攻反贼董卓，势单力薄，被董卓干掉了。**

从这个场景中，我们看到内奸孙坚通过发送误导信息，非常容易地就干扰了刘备和曹操的作战计划，导致两位忠臣被逐一击破。这个现象就是二忠一判难题。那么主公袁绍该怎么解决这个问题？

#### **1.9.3.2 拜占庭问题解法** 

##### **解法一 原理**

就是**将袁绍也参与进来进行投票**，这样就‍‍‍‍‍‍‍‍‍‍增加了一位忠臣的数量‍‍‍‍‍‍‍‍‍‍。三个忠臣一个叛贼。然后 4 位将军做了一个约定，如果没有收到命令，则执行默认命令，比如撤退。另外约定流程来发送作战信息和如何执行作战指令。这个**解法的关键点就是执行两轮作战信息协商。**

###### 1.9.3.2.1 袁绍作为指挥官

我们来看下第一轮是怎么做的。

1. 先发送作战信息的将军我们把他称为指挥官（袁绍），另外的将军我们称作副官（刘备，曹操，孙坚）。
2. 指挥官将他的作战信息发送给所有的副官。
3. **每一位副官将从指挥官处收到的作战信息，作为自己的作战指令；假如没有收到指挥官的作战信息，将把默认的撤退作为作战指令。**

我们用图来演示：袁绍作为主公先发送作战信息，作战指令为进攻。然后曹操、刘备、孙坚收到进攻的作战指令。

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzE4znkdZiaicgmF2az4XDBn3KLRw2gWuiacbnuIickV9rD11qJYQic6CTnJg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

第一轮

再来看下第二轮是怎么做的。

1. 第一轮指挥官（袁绍）已经发送指令了，现在就需要刘备、曹操、孙坚依次作为指挥官给其他两位副将发送作战信息。
2. 然后这三位副将按照少数服从多数的原则，执行收到的作战指令。

**孙坚使诈 - 两撤退**

如果孙坚使诈，比如给曹操和刘备都发送撤退信息，如下图所示。那么刘备和曹操收到的作战信息为 进攻 2 票，撤退 1 票，按照少数服从多数的原则，**最后刘备和曹操执行进攻，实现了作战计划的一致性，曹操和刘备联合作战击败了反贼董卓（即使孙坚没有参加作战。）**

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0Zmcpgzice9PdINIm0HP6mcSR3XDMKF4LSibD8POzWxhZhhpiaefhPAq6ntoBu7A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

孙坚使诈 - 两撤退

**孙坚使诈 - 一进一退**

假如孙坚使诈，给曹操发送撤退指令，给刘备发送进攻指令，那么刘备收到的作战信息是进攻 3 票，肯定会发起进攻了，而曹操收到的作战信息是进攻 2 票，撤退 1 票，最后曹操还是会进攻，所以**刘备和曹操还是联合作战击败了反贼董卓。**

如此看来，引入了一位指挥官后，确实可以避免孙坚使诈，但如果是孙坚在第一轮作为指挥官，其他人作为副官呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzoHm6De0ztUOp7bcG9Zf8OqSpsCbSooFSXTo90NtWe3GO3Q2Gcw3qaA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

孙坚使诈 - 一进一退

###### 1.9.3.2.2 孙坚作为指挥官

第一轮孙坚向其中一个副官袁绍发送撤退指令，向另外两个副官曹操、刘备发送进攻指令。那么第一轮的结果如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0ZmcpgzkeV6cF34YgF1gm6OSqZlqgv3pVKBeSkWrBFjSib6WsviaeOb6LnCDibSw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​                                                                                          第一轮

第二轮孙坚休息，其他副官按照孙坚发送的指令开始向另外的副官发送指令。

1. 曹操向刘备和袁绍发送进攻指令。
2. 刘备向曹操和袁绍发送进攻指令。
3. 袁绍向曹操和刘备发送撤退指令。

如下图所示，最后曹操、刘备、袁绍收到的指令为进攻 2 票，撤退 1 票，**按照少数服从多数原则，三个人都是发起进攻。执行了一致的作战计划，保证作战的胜利。**

![图片](https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ3EponJUvecQg0gQ0Zmcpgzo0ajBNibUZxar5qTGKnwdQLzWnibfcLfwBAO0FlQEUMFfYCtPlz3goAA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​                                                                         第二轮

###### **1.9.3.2.3 小结**

通过上面的演示，我们知道了如何解决拜占庭将军问题。其实兰伯特在他的论文中也提到过如何解决。

> 如果叛将人数为 m，将军数 n >= 3m + 1，那么就可以解决拜占庭将军问题。
>
> 前提条件：叛将数 m 一致，需要进行 m + 1 轮的作战协商。

这个公式，大家只需要记住就可以了，推到过程可以参考论文。

比如上述的攻打董卓问题，曹操、刘备、孙坚三个人当中，孙坚是叛将，他可以使诈，使作战计划不统一。必须增加一位忠臣袁绍来协商共识，才能达成一致性作战计划。

##### **解法二——签名**

那可以在不增加忠臣的情况下，解决拜占庭的二忠一判问题吗？

解法二就是通过签名消息。比如将军之间通过印章、虎符等信物进行通信。来保证这几个特征：

- 签名无法伪造，对签名消息的内容进行任何更改都会被发现。
- 任何人都能验证将军签名的真伪。

限于篇幅原因，签名的演示这里就不做展开了

#### **1.9.3.3 总结**

通过《三国杀》角色来讲解分布式中共识场景。那他们和分布式系统的映射关系是怎么样的呢？

- 将军对应计算机节点。
- 忠臣的将军对应正常运行的计算机节点。
- 叛变的将军对应出现故障并会发送误导信息的计算机节点。
- 信使被杀对应通讯故障、信息丢失。
- 信使被间谍替换对应为通讯被恶意攻击、伪造信息或劫持通讯。

可不要小瞧拜占庭问题，它可是分布式场景最复杂的的故障场景。比如在数字货币的区块链技术中就有用到这些知识点。而且必须使用拜占庭容错算法（也就是 Byzantine Fault Tolerance，BFT）。

拜占庭容错算法还有 FBFT 算法，PoW 算法，当然不会在这篇中去讲这些算法，后续再讲解。一口吃不了大胖子~

有了拜占庭容错算法，肯定有非拜占庭容错算法，顾名思义，就是没有发送误导信息的节点。CFT 算法就是解决分布式系统中存在故障，但不存在恶意节点的场景下的共识问题。简单来说就是可能**因系统故障造成丢失消息或消息重复，但不存在错误消息、伪造消息。**对应的算法有 Paxos 算法、Raft 算法、ZAB 协议。后续讲解~上面提到了 5 种算法，居然都是跟拜占庭问题有关，你说今天讲的拜占庭问题重要不重要？

这么多算法该如何选择？

**节点可信，选非拜占庭容错算法。否则就用拜占庭容错算法，如区块链中用到的 PoW 算法。**

## **1.10 八大分布式一致性协议和算法**

1. Paxos 算法
2. Raft 算法
3. 一致性 Hash 算法
4. Gossip 协议算法
5. Quorum NWR 算法
6. FBFT 算法
7. POW 算法
8. ZAB 协议
9. Lease机制--->1.6已经提到

分布式协议

https://blog.csdn.net/liushengxi_root/article/details/96302017
https://zhuanlan.zhihu.com/p/130332285
https://mp.weixin.qq.com/s/6IBp5ZoPAgMRLc1WukBLDQ

### 1.10.1 前言

#### **1.10.1.1 为什么需要一致性**

1. 数据不能存在单个节点（主机）上，否则可能出现单点故障。
2. 多个节点（主机）需要保证具有相同的数据。
3. 一致性算法就是为了解决上面两个问题。

#### **1.10.1.2 一致性算法的定义**

*一致性*就是数据保持*一致*，在分布式系统中，可以理解为多个节点中数据的值是*一致*的。

#### **1.10.1.3 一致性的分类**

- 强一致性

- - 说明：保证系统改变提交以后立即改变集群的状态。

  - 模型：

  - - Paxos
    - Raft（muti-paxos）
    - ZAB（muti-paxos）

- 弱一致性

- - 说明：也叫最终一致性，系统不保证改变提交以后立即改变集群的状态，但是随着时间的推移最终状态是一致的。

  - 模型：

  - - DNS系统
    - Gossip协议

也可以分为共享内存和消息传递两种方式

#### **1.10.1.4 一致性算法实现举例**

- Google的Chubby分布式锁服务，采用了Paxos算法
- etcd分布式键值数据库，采用了Raft算法
- ZooKeeper分布式应用协调服务，Chubby的开源实现，采用ZAB算法

##### 1.10.1.4.1 Zookeeper一致性的原理

原子广播

#### 1.10.1.5 非拜占庭条件/拜占庭错误

故障（不响应）即信道不可靠的情况称为”非拜占庭错误“，如**网络延迟，分区和数据包丢失，重复和乱序**等，解决办法：部分一致性算法、租约机制、幂等性算法、心跳机制

**恶意响应**（即系统被攻击）称为”拜占庭错误“，解决方法：数字签名、投票、部分一致性算法

### 1.10.2 一致性Hash算法

### **1.10.3 Paxos算法**

Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。

Paxos在原作者的《Paxos Made Simple》中内容是比较精简的

# 2. 分布式事务原理

## **2.1 前言**

从 CPU 到内存、到磁盘、到操作系统、到网络，计算机系统处处存在不可靠因素。工程师和科学家努力使用各种软硬件方法对抗这种不可靠因素，保证数据和指令被正确地处理。在网络领域有 **TCP 可靠传输协议**、在存储领域有 **Raid5** 和 **Raid6** 算法、在数据库领域有 **基于 ARIES 算法理论实现的事务机制**……

这篇文章先介绍**单机数据库事务的 ACID 特性**，然后指出**分布式场景下操作多数据源面临的困境**，引出**分布式系统中常用的分布式事务解决方案**，这些解决方案可以保证业务代码在操作多个数据源的时候，能够像操作单个数据源一样，具备 ACID 特性。

文章在最后给出业界较为成熟的分布式事务框架——**Seata 的 AT 模式全局事务的实现**。

## **2.2 单数据源事务 & 多数据源事务**

如果一个应用程序在一次业务流中通过连接驱动和数据源接口只连接并查询（这里的查询是广义的，包括增删查改等）一个特定的数据库，该应用程序就可以利用数据库提供的事务机制（如果数据库支持事务的话）保证对库中记录所进行的操作的可靠性，这里的可靠性有四种语义：

- 原子性，A
- 一致性，C
- 隔离性，I
- 持久性，D

笔者在这里不再对这四种语义进行解释，了解单数据源事务及其 ACID 特性是读者阅读这篇文章的前提。单个数据库实现自身的事务特性是一个复杂又微妙的过程，例如 **MySQL 的 InnoDB 引擎通过 Undo Log + Redo Log + ARIES 算法来实现**。

这是一个很宏大的话题，不在本文的描述范围，读者有兴趣的话可自行研究。

**单数据源事务也可以叫做单机事务，或者本地事务。**

在分布式场景下，一个系统由多个子系统构成，每个子系统有独立的数据源。多个子系统之间通过互相调用来组合出更复杂的业务。

在时下流行的微服务系统架构中，每一个子系统被称作一个微服务，同样每个微服务都维护自己的数据库，以保持独立性。

例如，一个电商系统可能由购物微服务、库存微服务、订单微服务等组成。购物微服务通过调用库存微服务和订单微服务来整合出购物业务。用户请求购物微服务商完成下单时，购物微服务一方面调用库存微服务扣减相应商品的库存数量，另一方面调用订单微服务插入订单记录（为了后文描述分布式事务解决方案的方便，这里给出的是一个最简单的电商系统微服务划分和最简单的购物业务流程，后续的支付、物流等业务不在考虑范围内）。电商系统模型如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\电商系统举例.png" style="zoom:50%;" />

在用户购物的业务场景中，**shopping-service** 的业务涉及两个数据库：**库存库（repo_db）\**和\**订单库（repo_db）**，也就是 g 购物业务是调用多数据源来组合而成的。作为一个面向消费者的系统，电商系统要保证购物业务的高度可靠性，这里的可靠性同样有 ACID 四种语义。

但是一个数据库的本地事务机制仅仅对落到自己身上的查询操作（这里的查询是广义的，包括增删改查等）起作用，无法干涉对其他数据库的查询操作。所以，**数据库自身提供的本地事务机制无法确保业务对多数据源全局操作的可靠性。**

基于此，针对多数据源操作提出的分布式事务机制就出现了。

**分布式事务也可以叫做全局事务。**

## **2.3 常见分布式事务解决方案**

### 2.3.1 分布式事务模型

描述分布式事务，常常会使用以下几个名词：

- **事务参与者**：例如每个数据库就是一个事务参与者
- **事务协调**者：访问多个数据源的服务程序，例如 shopping-service 就是事务协调者
- **资源管理器（Resource Manager, RM）**：通常与事务参与者同义
- **事务管理器（Transaction Manager, TM）**：通常与事务协调者同义

在分布式事务模型中，一个 TM 管理多个 RM，即一个服务程序访问多个数据源；TM 是一个全局事务管理器，协调多方本地事务的进度，使其共同提交或回滚，最终达成一种全局的 ACID 特性。

### 2.3.2 二将军问题和幂等性

#### 2.3.2.1 二将军

二将军问题是网络领域的一个经典问题，用于表达计算机网络中互联协议设计的微妙性和复杂性。

这里给出一个二将军问题的简化版本：

```
一支白军被围困在一个山谷中，山谷的左右两侧是蓝军。困在山谷中的白军人数多于山谷两侧的任意一支蓝军，而少于两支蓝军的之和。若一支蓝军对白军单独发起进攻，则必败无疑；但若两支蓝军同时发起进攻，则可取胜。两只蓝军的总指挥位于山谷左侧，他希望两支蓝军同时发起进攻，这样就要把命令传到山谷右侧的蓝军，以告知发起进攻的具体时间。假设他们只能派遣士兵穿越白军所在的山谷（唯一的通信信道）来传递消息，那么在穿越山谷时，士兵有可能被俘虏。
```

只有当送信士兵成功往返后，总指挥才能确认这场战争的胜利（上方图）。现在问题来了，派遣出去送信的士兵没有回来，则左侧蓝军中的总指挥能不能决定按命令中约定的时间发起进攻？

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\二将军1.webp" style="zoom:50%;" />

答案是不确定，派遣出去送信的士兵没有回来，他可能遇到两种状况：

- 命令还没送达就被俘虏了（中间图），这时候右侧蓝军根本不知道要何时进攻；
- 命令送达，但返回途中被俘虏了（下方图），这时候右侧蓝军知道要何时进攻，但左侧蓝军不知道右侧蓝军是否知晓进攻时间。

类似的问题在计算机网络中普遍存在，例如发送者给接受者发送一个 HTTP 请求，或者 MySQL 客户端向 MySQL 服务器发送一条插入语句，然后超时了没有得到响应。请问服务器是写入成功了还是失败了？答案是不确定，有以下几种情况：

- 可能请求由于网络故障根本没有送到服务器，因此写入失败；
- 可能服务器收到了，也写入成功了，但是向客户端发送响应前服务器宕机了；
- 可能服务器收到了，也写入成功了，也向客户端发送了响应，但是由于网络故障未送到客户端。

无论哪种场景，在客户端看来都是一样的结果：它发出的请求没有得到响应。为了确保服务端成功写入数据，客户端只能重发请求，直至接收到服务端的响应。

类似的问题问题被称为**网络二将军问题**

**网络二将军问题的存在使得消息的发送者往往要重复发送消息，直到收到接收者的确认才认为发送成功，但这往往又会导致消息的重复发送。** 例如电商系统中订单模块调用支付模块扣款的时候，如果网络故障导致二将军问题出现，扣款请求重复发送，产生的重复扣款结果显然是不能被接受的。因此要保证一次事务中的扣款请求无论被发送多少次，接收方有且只执行一次扣款动作，这种保证机制叫做接收方的幂等性。

#### 2.3.2.2 幂等性实现

所谓幂等性通俗的将就是一次请求和多次请求同一个资源产生相同的副作用

**1. 数据库去重表**

**2.MVCC方案**

**3. 状态机**

**4.select + insert**

并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法。

**5. TOKEN机制**

针对客户端连续点击或者调用方的超时重试等情况，例如提交订单，此种操作就可以用Token的机制实现防止重复提交。

**6.等等**

###  2.3.3 两阶段提交（2PC） & 三阶段提交（3PC）方案

**2PC 是一种实现分布式事务的简单模型**，这两个阶段是：

- **准备阶段**：事务协调者向各个事务参与者发起询问请求：“我要执行全局事务了，这个事务涉及到的资源分布在你们这些数据源中，分别是……，你们准备好各自的资源（即各自执行本地事务到待提交阶段）”。各个参与者协调者回复 yes（表示已准备好，允许提交全局事务）或 no（表示本参与者无法拿到全局事务所需的本地资源，因为它被其他本地事务锁住了）或超时。
- 提交阶段：如果各个参与者回复的都是 yes，则协调者向所有参与者发起事务提交操作，然后所有参与者收到后各自执行本地事务提交操作并向协调者发送 ACK；如果任何一个参与者回复 no 或者超时，则协调者向所有参与者发起事务回滚操作，然后所有参与者收到后各自执行本地事务回滚操作并向协调者发送 ACK。

2PC 的流程如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\二阶段提交.png" style="zoom:50%;" />

从上图可以看出，要实现 2PC，所有的参与者都要实现三个接口：

- **Prepare()**：TM 调用该接口询问各个本地事务是否就绪
- **Commit()**：TM 调用该接口要求各个本地事务提交
- **Rollback()**：TM 调用该接口要求各个本地事务回滚

可以将这三个接口简单地（但不严谨地）理解成 **XA 协议**。XA 协议是 X/Open 提出的分布式事务处理标准。MySQL、Oracle、DB2 这些主流数据库都实现了 XA 协议，因此都能被用于实现 2PC 事务模型。

2PC 简明易懂，但存在如下的问题：

- 性能差。在准备阶段，要等待所有的参与者返回，才能进入阶段二，在这期间，各个参与者上面的相关资源被排他地锁住，参与者上面意图使用这些资源的本地事务只能等待。因为存在这种同步阻塞问题，所以影响了各个参与者的本地事务并发度；
- 准备阶段完成后，如果协调者宕机，所有的参与者都收不到提交或回滚指令，导致所有参与者“不知所措”；
- 在提交阶段，协调者向所有的参与者发送了提交指令，如果一个参与者未返回 ACK，那么协调者不知道这个参与者内部发生了什么（由于网络二将军问题的存在，这个参与者可能根本没收到提交指令，一直处于等待接收提交指令的状态；也可能收到了，并成功执行了本地提交，但返回的 ACK 由于网络故障未送到协调者上），也就无法决定下一步是否进行全体参与者的回滚。

2PC 之后又出现了 3PC，把两阶段过程变成了三阶段过程，分别是：**1.询问阶段**、**2.准备阶段**、**3.提交或回滚阶段**，这里不再详述。

2PC 之后又出现了 3PC，把两阶段过程变成了三阶段过程，分别是：**1.询问阶段**、**2.准备阶段**、**3.提交或回滚阶段**，这里不再详述。

**3PC 利用超时机制解决了 2PC 的同步阻塞问题，避免资源被永久锁定，进一步加强了整个事务过程的可靠性。但是 3PC 同样无法应对类似的宕机问题，只不过出现多数据源中数据不一致问题的概率更小。**

2PC 除了性能和可靠性上存在问题，它的适用场景也很局限，它要求参与者实现了 XA 协议，例如使用实现了 XA 协议的数据库作为参与者可以完成 2PC 过程。但是在多个系统服务利用 api 接口相互调用的时候，就不遵守 XA 协议了，这时候 2PC 就不适用了。所以 2PC 在分布式应用场景中很少使用。

所以前文提到的电商场景无法使用 2PC，因为 shopping-service 通过 RPC 接口或者 Rest 接口调用 repo-service 和 order-service 间接访问 repo_db 和 order_db。除非 shopping-service 直接配置 repo_db 和 order_db 作为自己的数据库。

### 2.3.4 TCC 方案

描述 TCC 方案使用的电商微服务模型如下图所示，在这个模型中，shopping-service 是事务协调者，repo-service 和 order-service 是事务参与者。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\电商系统举例.png" style="zoom:50%;" />

上文提到，2PC 要求参与者实现了 XA 协议，通常用来解决多个数据库之间的事务问题，比较局限。在多个系统服务利用 api 接口相互调用的时候，就不遵守 XA 协议了，这时候 2PC 就不适用了。现代企业多采用分布式的微服务，因此更多的是要解决多个微服务之间的分布式事务问题。

TCC 就是一种解决多个微服务之间的分布式事务问题的方案。TCC 是 Try、Confirm、Cancel 三个词的缩写，其本质是一个应用层面上的 2PC，同样分为两个阶段：

- **准备阶段** ：协调者调用所有的每个微服务提供的 try 接口，将整个全局事务涉及到的资源锁定住，若锁定成功 try 接口向协调者返回 yes。
- **提交阶段** ：若所有的服务的 try 接口在阶段一都返回 yes，则进入提交阶段，协调者调用所有服务的 confirm 接口，各个服务进行事务提交。如果有任何一个服务的 try 接口在阶段一返回 no 或者超时，则协调者调用所有服务的 cancel 接口。

TCC 的流程如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\TCC1.png" style="zoom:50%;" />

这里有个关键问题，既然 TCC 是一种服务层面上的 2PC。**它是如何解决 2PC 无法应对宕机问题的缺陷的呢？**

答案是**不断重试**。

由于 try 操作锁住了全局事务涉及的所有资源，保证了业务操作的所有前置条件得到满足，因此无论是 confirm 阶段失败还是 cancel 阶段失败都能通过不断重试直至 confirm 或 cancel 成功（所谓成功就是所有的服务都对 confirm 或者 cancel 返回了 ACK）。

这里还有个关键问题，在不断重试 confirm 和 cancel 的过程中（考虑到网络二将军问题的存在）有可能重复进行了 confirm 或 cancel，因此还要再保证 confirm 和 cancel 操作具有幂等性，也就是整个全局事务中，每个参与者只进行一次 confirm 或者 cancel。实现 confirm 和 cancel 操作的幂等性，有很多解决方案，例如每个参与者可以维护一个去重表（可以利用数据库表实现也可以使用内存型 KV 组件实现），记录每个全局事务（以全局事务标记 XID 区分）是否进行过 confirm 或 cancel 操作，若已经进行过，则不再重复执行。

TCC 由支付宝团队提出，被广泛应用于金融系统中。我们用银行账户余额购买基金时，会注意到银行账户中用于购买基金的那部分余额首先会被冻结，由此我们可以猜想，这个过程大概就是 TCC 的第一阶段。

### 2.3.5 事务状态表方案

另外有一种类似 TCC 的事务解决方案，借助事务状态表来实现。假设要在一个分布式事务中实现调用 repo-service 扣减库存、调用 order-service 生成订单两个过程。在这种方案中，协调者 shopping-service 维护一张如下的事务状态表：

| 分布式事务 ID   | 事务内容                                                     | 事务状态                                                |
| :-------------- | :----------------------------------------------------------- | :------------------------------------------------------ |
| global_trx_id_1 | 操作 1：调用 repo-service 扣减库存 操作 2：调用 order-service 生成订单 | 状态 1：初始 状态 2：操作 1 成功 状态 3：操作 1、2 成功 |

初始状态为 1，每成功调用一个服务则更新一次状态，最后所有的服务调用成功，状态更新到 3。

有了这张表，就可以启动一个后台任务，扫描这张表中事务的状态，如果一个分布式事务一直（设置一个事务周期阈值）未到状态 3，说明这条事务没有成功执行，于是可以重新调用 repo-service 扣减库存、调用 order-service 生成订单。直至所有的调用成功，事务状态到 3。

如果多次重试仍未使得状态到 3，可以将事务状态置为 error，通过人工介入进行干预。

由于存在服务的调用重试，因此每个服务的接口要根据全局的分布式事务 ID 做幂等，原理同 2.4 节的幂等性实现。

### 2.3.6 基于消息中间件的最终一致性事务方案

**无论是 2PC & 3PC 还是 TCC、事务状态表，基本都遵守 XA 协议的思想**。即这些方案本质上都是事务协调者协调各个事务参与者的本地事务的进度，使所有本地事务共同提交或回滚，最终达成一种全局的 ACID 特性。在协调的过程中，协调者需要收集各个本地事务的当前状态，并根据这些状态发出下一阶段的操作指令。

但是这些全局事务方案由于操作繁琐、时间跨度大，或者在全局事务期间会排他地锁住相关资源，使得整个分布式系统的全局事务的并发度不会太高。这很难满足电商等高并发场景对事务吞吐量的要求，因此互联网服务提供商探索出了很多与 XA 协议背道而驰的分布式事务解决方案。其中利用消息中间件实现的最终一致性全局事务就是一个经典方案。

为了表现出这种方案的精髓，我将使用如下的电商系统微服务结构来进行描述：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\电商微服务结构.png" style="zoom:50%;" />

在这个模型中，用户不再是请求整合后的 shopping-service 进行下单，而是直接请求 order-service 下单，order-service 一方面添加订单记录，另一方面会调用 repo-service 扣减库存。

这种基于消息中间件的最终一致性事务方案常常被误解成如下的实现方式：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\MQ分布式事务.png" style="zoom:50%;" />

这种实现方式的流程是：

1. order-service 负责向 MQ server 发送扣减库存消息（repo_deduction_msg）；repo-service 订阅 MQ server 中的扣减库存消息，负责消费消息。
2. 用户下单后，order-service 先执行插入订单记录的查询语句，后将 repo_deduction_msg 发到消息中间件中，这两个过程放在一个本地事务中进行，一旦“执行插入订单记录的查询语句”失败，导致事务回滚，“将 repo_deduction_msg 发到消息中间件中”就不会发生；同样，一旦“将 repo_deduction_msg 发到消息中间件中”失败，抛出异常，也会导致“执行插入订单记录的查询语句”操作回滚，最终什么也没有发生。
3. repo-service 接收到 repo_deduction_msg 之后，先执行库存扣减查询语句，后向 MQ sever 反馈消息消费完成 ACK，这两个过程放在一个本地事务中进行，一旦“执行库存扣减查询语句”失败，导致事务回滚，“向 MQ sever 反馈消息消费完成 ACK”就不会发生，MQ server 在 Confirm 机制的驱动下会继续向 repo-service 推送该消息，直到整个事务成功提交；同样，一旦“向 MQ sever 反馈消息消费完成 ACK”失败，抛出异常，也对导致“执行库存扣减查询语句”操作回滚，MQ server 在 Confirm 机制的驱动下会继续向 repo-service 推送该消息，直到整个事务成功提交。

这种做法看似很可靠。但没有考虑到网络二将军问题的存在，有如下的缺陷：

- **网络的 2 将军问题** ：上面第 2 步中 order-service 发送 repo_deduction_msg 消息失败，对于发送方 order-service 来说，可能是消息中间件没有收到消息；也可能是中间件收到了消息，但向发送方 order-service 响应的 ACK 由于网络故障没有被 order-service 收到。因此 order-service 贸然进行事务回滚，撤销“执行插入订单记录的查询语句”，是不对的，因为 repo-service 那边可能已经接收到 repo_deduction_msg 并成功进行了库存扣减，这样 order-service 和 repo-service 两方就产生了数据不一致问题。
- **数据库长事务问题** ：repo-service 和 order-service 把网络调用（与 MQ server 通信）放在本地数据库事务里，可能会因为网络延迟产生数据库长事务，影响数据库本地事务的并发度。

以上是被误解的实现方式，下面给出正确的实现方式，如下所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\MQ事务2.webp" style="zoom:50%;" />

上图所示的方案，利用消息中间件如 rabbitMQ 来实现分布式下单及库存扣减过程的最终一致性。对这幅图做以下说明：

1）order-service 中，

```
在 t_order 表添加订单记录 &&

在 t_local_msg 添加对应的扣减库存消息
```

这两个过程要在一个事务中完成，保证过程的原子性。同样，repo-service 中，

```
检查本次扣库存操作是否已经执行过 &&

执行扣减库存如果本次扣减操作没有执行过 &&

写判重表 &&

向 MQ sever 反馈消息消费完成 ACK
```

这四个过程也要在一个事务中完成，保证过程的原子性。

2）order-service 中有一个后台程序，源源不断地把消息表中的消息传送给消息中间件，成功后则删除消息表中对应的消息。如果失败了，也会不断尝试重传。由于存在网络 2 将军问题，即当 order-service 发送给消息中间件的消息网络超时时，这时候消息中间件可能收到了消息但响应 ACK 失败，也可能没收到，order-service 会再次发送该消息，直至消息中间件响应 ACK 成功，这样可能发生消息的重复发送，不过没关系，只要保证消息不丢失，不乱序就行，后面 repo-service 会做去重处理。

3）消息中间件向 repo-service 推送 repo_deduction_msg，repo-service 成功处理完成后会向中间件响应 ACK，消息中间件收到这个 ACK 才认为 repo-service 成功处理了这条消息，否则会重复推送该消息。但是有这样的情形：repo-service 成功处理了消息，向中间件发送的 ACK 在网络传输中由于网络故障丢失了，导致中间件没有收到 ACK 重新推送了该消息。这也要靠 repo-service 的消息去重特性来避免消息重复消费。

4）在 2）和 3）中提到了两种导致 repo-service 重复收到消息的原因，一是生产者重复生产，二是中间件重传。为了实现业务的幂等性，repo-service 中维护了一张判重表，这张表中记录了被成功处理的消息的 id。repo-service 每次接收到新的消息都先判断消息是否被成功处理过，若是的话不再重复处理。

通过这种设计，实现了消息在发送方不丢失，消息在接收方不被重复消费，联合起来就是消息不漏不重，严格实现了 order-service 和 repo-service 的两个数据库中数据的最终一致性。

基于消息中间件的最终一致性全局事务方案是互联网公司在高并发场景中探索出的一种创新型应用模式，利用 MQ 实现微服务之间的异步调用、解耦合和流量削峰，支持全局事务的高并发，并保证分布式数据记录的最终一致性。

### **2.3.7 Seata in AT mode 的实现**

第 2 章给出了实现实现分布式事务的集中常见的理论模型。本章给出业界开源分布式事务框架 Seata 的实现。

**Seata 为用户提供了 AT、TCC、SAGA 和 XA 事务模式**。其中 **AT 模式是 Seata 主推的事务模式**，因此本章分析 Seata in AT mode 的实现。**使用 AT 有一个前提，那就是微服务使用的数据库必须是支持事务的关系型数据库。**

#### 2.3.7.1 Seata in AT mode 工作流程概述

Seata 的 AT 模式建立在关系型数据库的本地事务特性的基础之上，通过数据源代理类拦截并解析数据库执行的 SQL，记录自定义的回滚日志，如需回滚，则重放这些自定义的回滚日志即可。AT 模式虽然是根据 XA 事务模型（2PC）演进而来的，但是 AT 打破了 XA 协议的阻塞性制约，在一致性和性能上取得了平衡。

AT 模式是基于 XA 事务模型演进而来的，它的整体机制也是一个改进版本的两阶段提交协议。AT 模式的两个基本阶段是：

- 首先获取本地锁，执行本地事务，业务数据操作和记录回滚日志在同一个本地事务中提交，最后释放本地锁；
- 如需全局提交，异步删除回滚日志即可，这个过程很快就能完成。如需要回滚，则通过第一阶段的回滚日志进行反向补偿。

本章描述 Seata in AT mode 的工作原理使用的电商微服务模型如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\电商系统举例.png" style="zoom:50%;" />

在上图中，协调者 shopping-service 先调用参与者 repo-service 扣减库存，后调用参与者 order-service 生成订单。这个业务流使用 Seata in XA mode 后的全局事务流程如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\seata事务.webp" style="zoom:50%;" />

上图描述的全局事务执行流程为：

1. shopping-service 向 Seata 注册全局事务，并产生一个全局事务标识 XID
2. 将 repo-service.repo_db、order-service.order_db 的本地事务执行到待提交阶段，事务内容包含对 repo-service.repo_db、order-service.order_db 进行的查询操作以及写每个库的 undo_log 记录
3. repo-service.repo_db、order-service.order_db 向 Seata 注册分支事务，并将其纳入该 XID 对应的全局事务范围
4. 提交 repo-service.repo_db、order-service.order_db 的本地事务
5. repo-service.repo_db、order-service.order_db 向 Seata 汇报分支事务的提交状态
6. Seata 汇总所有的 DB 的分支事务的提交状态，决定全局事务是该提交还是回滚
7. Seata 通知 repo-service.repo_db、order-service.order_db 提交/回滚本地事务，若需要回滚，采取的是补偿式方法

其中 1）2）3）4）5）属于第一阶段，6）7）属于第二阶段。

#### 2.3.7.2 Seata in AT mode 工作流程详述

在上面的电商业务场景中，购物服务调用库存服务扣减库存，调用订单服务创建订单，显然这两个调用过程要放在一个事务里面。即：

```
start global_trx

 call 库存服务的扣减库存接口

 call 订单服务的创建订单接口

commit global_trx
```

在库存服务的数据库中，存在如下的库存表 t_repo：

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10001 | 20001           | xx 键盘 | 98    | 200.0 |
| 10002 | 20002           | yy 鼠标 | 199   | 100.0 |

在订单服务的数据库中，存在如下的订单表 t_order：

| id    | order_code    | user_id | production_code | count | price |
| :---- | :------------ | :------ | :-------------- | :---- | :---- |
| 30001 | 2020102500001 | 40001   | 20002           | 1     | 100.0 |
| 30002 | 2020102500001 | 40001   | 20001           | 2     | 400.0 |

现在，id 为 40002 的用户要购买一只商品代码为 20002 的鼠标，整个分布式事务的内容为：

1）在库存服务的库存表中将记录

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10002 | 20002           | yy 鼠标 | 199   | 100.0 |

修改为

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10002 | 20002           | yy 鼠标 | 198   | 100.0 |

2）在订单服务的订单表中添加一条记录

| id    | order_code    | user_id | production_code | count | price |
| :---- | :------------ | :------ | :-------------- | :---- | :---- |
| 30003 | 2020102500002 | 40002   | 20002           | 1     | 100.0 |

以上操作，在 AT 模式的第一阶段的流程图如下：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\AT模式流程1.webp" style="zoom:50%;" />

从 AT 模式第一阶段的流程来看，分**支的本地事务在第一阶段提交完成之后，就会释放掉本地事务锁定的本地记录**。这是 AT 模式和 XA 最大的不同点，在 XA 事务的两阶段提交中，被锁定的记录直到第二阶段结束才会被释放。所以 **AT 模式减少了锁记录的时间，从而提高了分布式事务的处理效率**。

**AT 模式之所以能够实现第一阶段完成就释放被锁定的记录，是因为 Seata 在每个服务的数据库中维护了一张 undo_log 表**，其中记录了对 t_order / t_repo 进行操作前后记录的镜像数据，即便第二阶段发生异常，只需回放每个服务的 undo_log 中的相应记录即可实现全局回滚。

undo_log 的表结构：

| id   | branch_id   | xid         | context | rollback_info                                                | log_status | log_created | log_modified |
| :--- | :---------- | :---------- | :------ | :----------------------------------------------------------- | :--------- | :---------- | :----------- |
| ……   | 分支事务 ID | 全局事务 ID | ……      | 分支事务操作的记录在事务前后的记录镜像，即 beforeImage 和 afterImage | ……         | ……          | ……           |

第一阶段结束之后，Seata 会接收到所有分支事务的提交状态，然后决定是提交全局事务还是回滚全局事务。

**1）若所有分支事务本地提交均成功，则 Seata 决定全局提交。** Seata 将分支提交的消息发送给各个分支事务，各个分支事务收到分支提交消息后，会将消息放入一个缓冲队列，然后直接向 Seata 返回提交成功。之后，每个本地事务会慢慢处理分支提交消息，处理的方式为：删除相应分支事务的 undo_log 记录。之所以只需删除分支事务的 undo_log 记录，而不需要再做其他提交操作，是因为提交操作已经在第一阶段完成了（这也是 AT 和 XA 不同的地方）。这个过程如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\AT模式2.webp" style="zoom:50%;" />

分支事务之所以能够直接返回成功给 Seata，是因为真正关键的提交操作在第一阶段已经完成了，清除 undo_log 日志只是收尾工作，即便清除失败了，也对整个分布式事务不产生实质影响。

**2）若任一分支事务本地提交失败，则 Seata 决定全局回滚**，将分支事务回滚消息发送给各个分支事务，由于在第一阶段各个服务的数据库上记录了 undo_log 记录，分支事务回滚操作只需根据 undo_log 记录进行补偿即可。全局事务的回滚流程如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\AT模式2.webp" style="zoom:50%;" />

这里对图中的 2、3 步做进一步的说明：

- 由于上文给出了 undo_log 的表结构，所以可以通过 xid 和 branch_id 来找到当前分支事务的所有 undo_log 记录；
- 拿到当前分支事务的 undo_log 记录之后，首先要做数据校验，如果 afterImage 中的记录与当前的表记录不一致，说明从第一阶段完成到此刻期间，有别的事务修改了这些记录，这会导致分支事务无法回滚，向 Seata 反馈回滚失败；如果 afterImage 中的记录与当前的表记录一致，说明从第一阶段完成到此刻期间，没有别的事务修改这些记录，分支事务可回滚，进而根据 beforeImage 和 afterImage 计算出补偿 SQL，执行补偿 SQL 进行回滚，然后删除相应 undo_log，向 Seata 反馈回滚成功。

事务具有 ACID 特性，全局事务解决方案也在尽量实现这四个特性。以上关于 Seata in AT mode 的描述很显然体现出了 AT 的原子性、一致性和持久性。下面着重描述一下 AT 如何保证多个全局事务的隔离性的。

在 AT 中，当多个全局事务操作同一张表时，通过全局锁来保证事务的隔离性。下面描述一下全局锁在读隔离和写隔离两个场景中的作用原理：

#### 2.3.7.3 AT隔离性

##### **写隔离**

（若有全局事务在改/写/删记录，另一个全局事务对同一记录进行的改/写/删要被隔离起来，即写写互斥）：写隔离是为了在多个全局事务对同一张表的同一个字段进行更新操作时，避免一个全局事务在没有被提交成功之前所涉及的数据被其他全局事务修改。写隔离的基本原理是：在第一阶段本地事务（开启本地事务的时候，本地事务会对涉及到的记录加本地锁）提交之前，确保拿到全局锁。如果拿不到全局锁，就不能提交本地事务，并且不断尝试获取全局锁，直至超出重试次数，放弃获取全局锁，回滚本地事务，释放本地事务对记录加的本地锁。

假设有两个全局事务 gtrx_1 和 gtrx_2 在并发操作库存服务，意图扣减如下记录的库存数量：

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10002 | 20002           | yy 鼠标 | 198   | 100.0 |

AT 实现写隔离过程的时序图如下：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\写隔离.webp" style="zoom:50%;" />

图中，1、2、3、4 属于第一阶段，5 属于第二阶段。

在上图中 gtrx_1 和 gtrx_2 均成功提交，如果 gtrx_1 在第二阶段执行回滚操作，那么 gtrx_1 需要重新发起本地事务获取本地锁，然后根据 undo_log 对这个 id=10002 的记录进行补偿式回滚。此时 gtrx_2 仍在等待全局锁，且持有这个 id=10002 的记录的本地锁，因此 gtrx_1 会回滚失败（gtrx_1 回滚需要同时持有全局锁和对 id=10002 的记录加的本地锁），回滚失败的 gtrx_1 会一直重试回滚。直到旁边的 gtrx_2 获取全局锁的尝试次数超过阈值，gtrx_2 会放弃获取全局锁，发起本地回滚，本地回滚结束后，自然会释放掉对这个 id=10002 的记录加的本地锁。此时，gtrx_1 终于可以成功对这个 id=10002 的记录加上了本地锁，同时拿到了本地锁和全局锁的 gtrx_1 就可以成功回滚了。整个过程，全局锁始终在 gtrx_1 手中，并不会发生脏写的问题。整个过程的流程图如下所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\写隔离2.webp" style="zoom:50%;" />

##### **读隔离**

（若有全局事务在改/写/删记录，另一个全局事务对同一记录的读取要被隔离起来，即读写互斥）：在数据库本地事务的隔离级别为读已提交、可重复读、串行化时（读未提交不起什么隔离作用，一般不使用），Seata AT 全局事务模型产生的隔离级别是读未提交，也就是说一个全局事务会看到另一个全局事务未全局提交的数据，产生脏读，从前文的第一阶段和第二阶段的流程图中也可以看出这一点。这在最终一致性的分布式事务模型中是可以接受的。

如果要求 AT 模型一定要实现读已提交的事务隔离级别，可以利用 Seata 的 SelectForUpdateExecutor 执行器对 SELECT FOR UPDATE 语句进行代理。SELECT FOR UPDATE 语句在执行时会申请全局锁，如果全局锁已经被其他全局事务占有，则回滚 SELECT FOR UPDATE 语句的执行，释放本地锁，并且重试 SELECT FOR UPDATE 语句。在这个过程中，查询请求会被阻塞，直到拿到全局锁（也就是要读取的记录被其他全局事务提交），读到已被全局事务提交的数据才返回。这个过程如下图所示：

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\读隔离.webp" style="zoom:50%;" />

## 2.4 总结

XA 协议是 X/Open 提出的分布式事务处理标准。文中提到的 2PC、3PC、TCC、本地事务表、Seata in AT mode，无论哪一种，本质都是事务协调者协调各个事务参与者的本地事务的进度，使使所有本地事务共同提交或回滚，最终达成一种全局的 ACID 特性。在协调的过程中，协调者需要收集各个本地事务的当前状态，并根据这些状态发出下一阶段的操作指令。这个思想就是 XA 协议的要义，我们可以说这些事务模型遵守或大致遵守了 XA 协议。

**基于消息中间件的最终一致性事务方案是互联网公司在高并发场景中探索出的一种创新型应用模式，利用 MQ 实现微服务之间的异步调用、解耦合和流量削峰，保证分布式数据记录的最终一致性。它显然不遵守 XA 协议。**

对于某项技术，可能存在业界标准或协议，但实践者针对具体应用场景的需求或者出于简便的考虑，给出与标准不完全相符的实现，甚至完全不相符的实现，这在工程领域是一种常见的现象。TCC 方案如此、基于消息中间件的最终一致性事务方案如此、Seata in AT mode 模式也如此。而新的标准往往就在这些创新中产生。

你难道真的没有发现 2.6 节（基于消息中间件的最终一致性事务方案）给出的正确方案中存在的业务漏洞吗？

## 2.5应用场景

### 2.5.1 服务内跨数据库

如下图所示，在同一个服务方法内，访问两个或两个以上数据库。

我们知道，Java事务是通过Connection对象控制的。不同的数据库，是不同的数据库链接，通过不同的Connection对象实现。传统数据库事务无法实现事务控制，需要引入事务协调者的概念。这是场景A，这个场景中分布式体现在数据库的部署上。

<img src="https://img-blog.csdn.net/2018061213415831" alt="img" style="zoom:50%;" />

### 2.5.2 跨内部服务

如下图所示，一个服务通过微服务框架或者RPC调用调用其他的服务，多个子服务需要同时成功或失败。每个子服务都有自己的持久化方式，不一定是数据库，体现事务的持久性。每个子服务部署在不同的服务容器中，不同的服务容器部署在不同的服务器节点上。这是场景B，这个场景中分布式体现在服务（或应用）的部署上。

这时候，事务的概念已经超出“数据库”的范畴了。

<img src="https://img-blog.csdn.net/20180612134212895" alt="img" style="zoom:50%;" />

### 2.5.3 跨外部服务

这个场景是在应用场景B的基础上，进一步，服务的具体实现在我们控制范围之外。我们不能限制其实现语言，不能要求指定方法上加标注（注解）。甚至除了服务调用的网络通道外，我们不能期望服务间访问相同的Zookeeper作为事务协调器。这是场景C，这个场景中，我们只能在通信协议层面做约定，是最彻底的分布式场景。

<img src="https://img-blog.csdn.net/20180612134232972" alt="img" style="zoom:50%;" />

# 3.高性能中间件

## 3.1 redis分布式缓存

### 3.1.1 为啥在项目里要用缓存？

用缓存，主要是两个用途：高性能 和 高并发

#### 高性能

假设有这么个场景，有一个操作，一个请求过来，然后执行N条SQL语句，然后半天才查询出一个结果，耗时600ms，但是这个结果可能接下来几个小时就不会变了，或者变了也可以不用立即反馈给用户，这个时候就可以使用缓存了。

我们可以把花费了600ms查询出来的数据，丢进缓存中，一个key对应一个value，下次再有人来查询的时候，就不走mysql了，而是直接从缓存中读取，通过key直接查询出value，耗时2ms，性能提升300倍。这就是所谓的高性能。

就是把一些复杂操作耗时查询出来的结果，如果确定后面不怎么变化了，但是马上还有很多读请求，这个时候，就可以直接把结果存放在缓存中，后面直接读取缓存即可。

<img src="C:/Users/felixsfan/Desktop/分布式/images/image-20200421122211630.png" alt="image-20200421122211630" style="zoom:50%;" />

就第一次从数据库中获取，后面直接从缓存中获取即可，性能提升很高

#### 高并发

MySQL这么重的数据库，并不适合于高并发，虽然可以使用，但是天然支持的就不好，因为MySQL的单机撑到2000QPS的时候，就容易报警了

<img src="C:/Users/felixsfan/Desktop/分布式/images/image-20200421124116765.png" alt="image-20200421124116765" style="zoom:50%;" />

##### 为什么缓存可以支持高并发

首先因为缓存是走内存的，内存天然就可以支持高并发，但是数据库因为是存储在硬盘上的，因此不要超过2000QPS

##### 场景

所以要是有一个系统，高峰期过来每秒的请求有1W个，要是MySQL单机的话，一定会宕机的，这个时候就只能用上缓存，把很多数据放到缓存中，这样请求过来了之后，就直接从缓存中获取数据，而不查询数据库。缓存的功能很简单，说白了就是一个 key - value式数据库，单机支撑的并发量轻松超过一秒几万 到 十多万，单机的承载量是mysql单机的几十倍。

#### 缓存带来的不良后果

场景的缓存问题有三个

- 缓存与数据库双写不一致的问题
- 缓存穿透
- 缓存雪崩
- 缓存并发竞争

### 3.1.2 **Redis主从复制模式** 

#### **3.1.2.1.** **Redis主从复制模式** 

Master可以有多个Slave。 

Slave也可以连接其它的Slave。 

Slave同步Master数据时，Master不会阻塞，可以继续处理client的读写请求。（乐观复制策略） 

Mater主库可以进行读写操作，Slave从库一般只是进行读操作。 

#### 3.1.2.2作用

​      1.读写分离 2.容灾备份

#### 3.1.2.3 **主从复制配置** 

##### 3.1.2.3.1配从不配主

##### 3.1.2.3.2从库配置

​        slaveof主库IP主库端口

​        每次与master断开之后，都需要重新连接，除非你配置进 redis.conf文件

##### 3.1.2.3.3修改配置细节操左

###### **3.1.2.3.3.1** **Master配置** 

​                           配置redis.conf

###### 3.1.2.3.3.2.Slave配置

​                            拷贝多个redis.conf文件

​                            开启daemonize  yes

​                            Pid文件名字

​                            指定端口

​                            Log文件名字

​                            Dump.rdb名字

##### 3.1.2.3.4三种模型方式

​                一主二仆

​               薪火相传

​               反客为主

#### 3.1.2.4主从复制

##### 3.1.2.4.1 **主从复制基本原理** 

l Slave启动时，向Master发送sync命令，2.8版本发送psync，以实现增量复制。 

l Mater(主库)接到sync请求后，会在后台保存快照，也就是实现RDB持久化，并将保存快照期间接收到的命令缓存起来。

l 快照完成后，Master(主库)会将快照文件和所有的缓存的命令发送给Slave(从库)。 

l Slave(从库)接收后，会载入快照文件并执行缓存的命令，从而完成复制的初始化。 

l 在数据库使用阶段，Master(主库)会自动把每次收到的写命令同步到从服务器。 

![](C:\Users\dell\Desktop\redis\images\redis主从复制的原理.png)

##### **3.1.2.4.**2 **Redis乐观复制策略** （弱一致性）

Redis采用乐观复制的策略，允许一定时间内主从数据库的内容不同，当然最终的数据会相同。此策略保证了Redis性能，在进行复制时，Master(主库)并不阻塞，继续处理client的请求。

但是Redis同样提供了配置用来限制**只有当数据库至少同步给指定数量的Slave(从库)时**，Master(主库)才可写，否则返回错误。配置是：min-slaves-to-write、min-slaves-max-lag。 

##### 3.1.2.4.3 主从复制的断点续传 

从redis 2.8开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份

master node会在内存中常见一个backlog，master和slave都会保存一个replica offset还有一个master id，offset就是保存在backlog中的。如果master和slave网络连接断掉了，slave会让master从上次的replica offset开始继续复制，但是如果没有找到对应的offset，那么就会执行一次resynchronization

##### 3.1.2.4.4 Redis主从复制的完整复制流程

###### 主从复制流程图

- slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始master host和ip是从哪儿来的，redis.conf里面的slaveof配置的
- slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接
- slave node发送ping命令给master node
- 口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证
- master node第一次执行全量复制，将所有数据发给slave node
- master node后续持续将写命令，异步复制给slave node

![](C:\Users\dell\Desktop\redis\images\复制的完整的基本流程.png)

###### 数据同步相关核心机制

指的就是第一次slave连接msater的时候，执行的全量复制，那个过程里面你的一些细节的机制

- master和slave都会维护一个offset

master会在自身不断累加offset，slave也会在自身不断累加offset
slave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset

这个倒不是说特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况

- backlog

master node有一个backlog，默认是1MB大小
master node给slave node复制数据时，也会将数据在backlog中同步写一份
backlog主要是用来做全量复制中断后的增量复制的

- master run id

info server，可以看到master run id
如果根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制
如果需要不更改run id重启redis，可以使用redis-cli debug reload命令

- psync

从节点使用psync从master node进行复制，psync runid offset master node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制

![](C:\Users\dell\Desktop\redis\images\maste run id的作用.png)

###### 全量复制

- master执行bgsave，在本地生成一份rdb快照文件
- master node将rdb快照文件发送给salve node，如果rdb复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调节大这个参数
- 对于千兆网卡的机器，一般每秒传输100MB，6G文件，很可能超过60s
- master node在生成rdb时，会将所有新的写命令缓存在内存中，在salve node保存了rdb之后，再将新的写命令复制给salve node
- client-output-buffer-limit slave 256MB 64MB 60，如果在复制期间，内存缓冲区持续消耗超过64MB，或者一次性超过256MB，那么停止复制，复制失败
- slave node接收到rdb之后，清空自己的旧数据，然后重新加载rdb到自己的内存中，同时基于旧的数据版本对外提供服务

rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite，很耗费时间

如果slave node开启了AOF，那么会立即执行BGREWRITEAOF，重写AOF

###### 增量复制

- 如果全量复制过程中，master-slave网络连接断掉，那么salve重新连接master时，会触发增量复制
- master直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB
- master就是根据slave发送的psync中的offset来从backlog中获取数据的

###### 异步复制

master每次接收到写命令之后，先在内部写入数据，然后异步发送给slave node

###### 心跳机制

master默认每隔10秒发送一次心跳，salve node每隔1秒发送一个心跳

### 3.1.3 哨兵模式

#### 3.1.3.1搭建

l 前面主从集群有个问题，就是Master(主库)挂了之后，无法重新选举新的节点作为主节点进行写操作，导致服务不可用。

l Redis提供了哨兵工具来实现监控Redis系统的运行情况，能够实现如下功能：

l 监控主从数据库运行是否正常。

l 当主数据库出现故障时，自动将从数据库转换为主数据库。

l 使用Redis-sentinel,redis实例必须在非集群模式下运行。

配置哨兵模式 - 配置sentinel.conf文件



**启动哨兵**：/opt/redis-shaobing/src/redis-sentinel /opt/redis-shaobing/sentinel.conf

**进入哨兵**：/opt/redis-shaobing/src/redis-cli -h 192.168.17.129 -p 26379

**查看哨兵信息**：/opt/redis-shaobing/src/redis-cli -h 192.168.17.129 -p 26379 info Sentinel

**停掉**Master，查看哨兵日志，****从****库重启后不能再次成为Master。

![img](C:\Users\dell\Desktop\redis\images\wps4.jpg) 

#### 3.1.3.2Redis主备切换的数据丢失问题：异步复制、集群脑裂

主备切换的过程，可能会导致数据丢失

##### 异步复制导致的数据丢失

因为master -> slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了。

![](C:\Users\dell\Desktop\redis\images\异步复制导致的数据丢失问题.png)

##### 脑裂导致的数据丢失

脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着，此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master

这个时候，集群里就会有两个master，也就是所谓的脑裂。此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了

因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据

![](C:\Users\dell\Desktop\redis\images\集群脑裂导致的数据丢失问题.png)

同时原来的master节点上的，client像 旧的 master中写入数据，当网络分区恢复正常后，client写的数据就会因为复制，导致数据的丢失。

##### 解决异步复制和脑裂导致数据丢失

```
min-slaves-to-write 1
min-slaves-max-lag 10
```

要求至少有1个slave，数据复制和同步的延迟不能超过10秒

如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了，上面两个配置可以减少异步复制和脑裂导致的数据丢失

- 减少异步复制的数据丢失

有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内

![](C:\Users\dell\Desktop\redis\images\异步复制导致数据丢失如何降低损失.png)

- 减少脑裂的数据丢失

如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求，这样脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失，上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求，因此在脑裂场景下，最多就丢失10秒的数据

![](C:\Users\dell\Desktop\redis\images\脑裂导致数据丢失的问题如何降低损失.png)

### 3.1.4 redis-cluster集群

#### **3.1.4.1** **集群介绍** 

Redis 集群是一个提供在多个Redis节点间共享数据的程序集。

Redis集群并不支持处理多个keys的命令(比如mset、mget等)，因为这需要在不同的节点间移动数据,从而达不到像Redis那样的性能,在高负载的情况下可能会导致不可预料的错误.

Redis 集群通过分区来提供一定程度的可用性，在实际环境中当某个节点宕机或者不可达的情况下继续处理命令。 

#### **3.1.4.2** **Redis 集群的优势** 

自动分割数据到不同的节点上。

整个集群的部分节点失败或者不可达的情况下能够继续处理命令。 

#### **3.1.4.3.** Redis 集群的数据分片

Redis集群引入了**哈希槽**的概念。

Redis 集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽。集群的每个节点负责一部分hash槽。

举个例子：比如当前集群有3个节点,那么：节点 A 包含 0 到 5500号哈希槽。节点 B 包含5501 到 11000 号哈希槽。节点 C 包含11001 到 16383号哈希槽。 

**这种结构很容易添加或者删除节点**： 比如我们想新添加个节点D, 我们需要将节点 A、B、 C中的部分槽移动到D上。如果我想移除节点A，需要将A中的槽移到B和C节点上。然后将没有任何槽的A节点从集群中移除即可。由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态.

#### **3.1.4.4** **Redis集群安装**

![](C:\Users\dell\Desktop\redis\images\22.png)

##### 3.1.4.4.1使用redis提供的rb脚本

**安装方式**：在一台机器上搭建集群

**安装节点数**：必须有3个或3个以上的主节点，所以采用3主3从方式安装；



**搭建伪集群：**redis启动时附带一个配置文件，配置文件只要端口号不一样就不会冲突，一个配置文件就是一个redis实例，配置多个端口号不同的配置文件然后根据不同的配置文件启动redis或者开多个虚拟机界面

###### **3.1.4.4.1.1** **创建目录**

**l** **在/opt下创建redis-cluster目录；**

**l** **在redis-cluster创建8000至8005六个目录；**

**l** **将redis.conf文件移动到8000-8005文件夹下；**

**3.4.4.1.2** **修改redis.conf文件**

每个redis.conf文件修改配置点：*表示要替换的字符：如8000、8001、8002、8003、8004、8005



###### **3.1.4.4.1.3** **按次序启动每一个节点**



###### **3.1.4.4.1.4** **创建集群**

Redis5.0将Ruby创建集群的方式改为了C语言创建，创建命令也进行了修改；

**执行下面的一条命令：**

/opt/redis-5.0.3/src/redis-cli --cluster create 192.168.179.128:8000 1192.168.179.128:8001 192.168.179.128:8002 192.168.179.128:8003 192.168.179.128:8004 192.168.179.128:8005 --cluster-replicas 1

**输入命令后，redis会自动划分hash槽，看下面日志，在划分完成后，输入yes即可**



##### 3.1.4.4.2原生搭建

###### 3.1.4.4.2.1配置开启cluster节点

  cluster-enabled yes(启动集群模式)

  cluster-config-file nodes-8001.conf(这里800x最好和port对应上)

###### 3.1.4.4.2.2meet

   cluster meet ip port

###### 3.1.4.4.2.3指派槽

   查看crc16算法算出key的槽位命令 cluster keyslot key

   16384/3 0-5461 5462-10922 10923-16383

   16384/4 4096

​    cluster addslots slot(槽位下标)

###### 3.1.4.4.2.4分配主从

​    cluster replicate node-id

#### 3.1.4.5集群的扩容缩容

​         扩容缩容槽位迁移数据也跟着迁移

##### 3.1.4.5.1扩容

​          ![](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\1587020577395.png)

##### 3.1.4.5.2缩容

​                ![](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\1587020602107.png)

1.redis集群解决了写操作无法负载均衡，

2.解决了存储能力受单机限制的问题，减少每个节点的数据存储量

### 3.1.5 Redis缓存和数据库一致性

#### 3.1.5.1实时同步

##### 3.1.5.1.1 双写一致性的策略

###### 先更新缓存，再更新数据库

更新缓存成功，更新数据库失败会产生脏数据

###### 先更新数据库，再更新缓存

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\双写一致1.webp" style="zoom:50%;" />

如上图redis更新失败则会造成数据不一致的情况,直到缓存超时自动删除或则下次更新才可能一致

###### 先删除缓存，再更新数据库

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\双写一致2.webp" style="zoom:50%;" />

线程更新完数据库和线程2存入的缓存值不一致

###### 先更新数据库，再删除缓存

线程A查询数据库 x=3

线程B更新数据库 x=4

线程B删除缓存

线程A更新旧值到缓存

##### 3.1.5.1.2 解决

延迟双删

出现删除失败

循环删除

##### 3.1.5.1.3 spring-cache 数据库一致性解决方案

​      对强一致性要求高的，应采用实时同步方案，即查询缓存查询不到再到数据库查询，保存到缓存；更新缓存时，先更新数据库，再将缓存的设置过期（建议不要去更新缓存内容，直接设置缓存过期）。

Spring缓存注解：

@Cacheable:存在缓存则直接返回,不存在则调用业务方法,保存到缓存

@CachePut：不管缓存存不存在,调用业务方法,将返回值set到缓存里面

@CacheEvict：清空缓存

@Caching：组合注解

原理：SpringAOP

![1587203185384](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\1587203185384.png)

Spring默认是内置缓存ConcurrentHashMap

可以配置缓存管理器来指定为redis

##### 3.1.5.1.4 存在的问题

 一致性问题

　　读数据的时候首先去Redis里读，没有读到再去MySQL里读，读回来之后更新到Redis里作为下一次的缓存。写数据的时候会产生数据不一致的问题，无论是先写到Redis里再写MySQL还是先写MySQL再写Redis，这两步写操作不能保证原子性，所以会出现Redis和MySQL里的数据不一致。无论采取何种方式都不能保证强一致性，如果对Redis里的数据设置了过期时间能够保证最终一致性，对架构做的优化只能降低不一致性发生的概率，不能从根本上避免不一致性。

​          <https://www.jianshu.com/p/7c4053b81ea2>

#### 3.1.5.2非实时同步

##### 3.1.5.2.1定时任务

​        一篇文章1分钟内被点击100万次，更新redis缓存，定时任务每天凌晨两点同步到数据库

##### 3.1.5.2.2异步队列

​         对于并发程度高的，可以采用异步队列的方式同步，可采用Kafka、rabbitMq等消息中间件处理消息生产和消费。RabbitMq优点可靠性高，可以做到消息0丢失。

#### 3.1.5.3阿里的同步工具canal

canal实现方式是模拟mysql slave和master的同步机制，监控DB bitlog的日志更新来触发缓存的更新，此方法可以解放程序员双手，减轻工作量，但使用时存在局限性；

Mysql主从复制原理参考下文MySQL

**canal的工作原理**

![1587136282683](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\1587136282683.png)

1.canal模拟mysql salve的交互协议，伪装自己为mysql slave，向mysqll master发送dump协议

2.mysql master 收到dump请求，开始推送binary log给slave(也就是canal)

3.canal解析binary log对象（原始为byte流）

#### 3.1.5.4采用UDF自定义函数的方式

面向mysql的API进行编程，利用触发器进行缓存同步，但UDF主要是C/C++语言实现，学习成本高

## 3.2 消息中间件

### 3.2.1 kafka

见MQ文件夹

## 3.3 中间接入层

### 3.3.1 Nginx

见Nginx文件夹

### 3.3.2 LVS

见LVS文件夹

### 3.3.3 Keepalived

### 3.3.4 Haproxy

# 4. 数据存储

## 4.1分布式数据库

### 4.1.1 理论基础

主从复制、读写分离、扩展读和写、高可用性等理论见数据库总结文档和《高性能MySQL》，这里只补充具体的技术

### 4.1.2 分库分表

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\分库分表.png)

### 4.1.3 分布式ID

见分布式ID.pdf

### 4.1.4 mycat数据库中间件



### 4.1.5 Mysql+Keepalived实现双主高可用



## 4.2 nosql

redis等

## 4.3 MongoDB

待完善

## 4.4 Hadoop所涉及的存储

所谓的“大数据”其实是海量数据采集清洗转换、数据存储、数据分析、数据服务等场景解决方案的一个统称，在每一个场景都包含了多种可选的技术，如数据采集有Flume、Sqoop、Kettle等，数据存储有分布式文件系统HDFS、FastDFS，NoSQL数据库HBase、MongoDB等，数据分析有Spark技术栈、机器学习算法等。总的来说大数据架构就是根据业务的需求，整合各种大数据组件组合而成的架构，一般会提供分布式存储、分布式计算、多维分析、数据仓库、机器学习算法等能力。

# 5.分布式服务调用

## 5.1 远程调用的发展过程

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\发展历史.png)

## 5.2 RPC理论

RPC(Remote Procedure Call)：远程过程调用，它是一种通过网络从远程计算机程序上调用服务，就像调用本地服务一样，而不需要了解底层网络技术的思想。

RPC 是一种技术思想而非一种规范或协议，**常见 RPC 技术和框架有**：

- 应用级的服务框架：阿里的 Dubbo/Dubbox、Google gRPC、Spring Boot/Spring Cloud。
- 远程通信协议：RMI、Socket、SOAP(HTTP XML)、REST(HTTP JSON)。
- 通信框架：MINA 和 Netty。

目前流行的开源 RPC 框架还是比较多的，有阿里巴巴的 Dubbo、Facebook 的 Thrift、Google 的 gRPC、Twitter 的 Finagle 等。

下面重点介绍三种：

- gRPC：是 Google 公布的开源软件，基于***的 HTTP 2.0 协议，并支持常见的众多编程语言。RPC 框架是基于 HTTP 协议实现的，底层使用到了 Netty 框架的支持。
- Thrift：是 Facebook 的开源 RPC 框架，主要是一个跨语言的服务开发框架。

用户只要在其之上进行二次开发就行，应用对于底层的 RPC 通讯等都是透明的。不过这个对于用户来说需要学习特定领域语言这个特性，还是有一定成本的。

- Dubbo：是阿里集团开源的一个极为出名的 RPC 框架，在很多互联网公司和企业应用中广泛使用。协议和序列化框架都可以插拔是极其鲜明的特色。

### **5.2.1 完整的 RPC 框架**

在一个典型 RPC 的使用场景中，包含了服务发现、负载、容错、网络传输、序列化等组件，其中“RPC 协议”就指明了程序如何进行网络传输和序列化。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\完整RPC架构图.jpg" style="zoom:50%;" />

如下是 Dubbo 的设计架构图，分层清晰，功能复杂：

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\dubbo架构图.jpg)

### **5.2.2 RPC 核心功能**

RPC 的核心功能是指实现一个 RPC 最重要的功能模块，就是上图中的”RPC 协议”部分：

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\RPC核心功能.jpg)

> **传输协议（用于数据传输）**：grpc使用HTTP2协议，dubbo使用自定义报文的TCP协议，还有 MQ
> **编码协议（用于数据序列化）**：基于文本编码的xml、json，基于二进制编码protobuf等

**一个 RPC 的核心功能主要有 5 个部分组成，分别是：客户端、客户端 Stub、网络传输模块、服务端 Stub、服务端等。**

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\RPC核心功能图.jpg)

下面分别介绍核心 RPC 框架的重要组成：

- 客户端(Client)：服务调用方。
- 客户端存根(Client Stub)：存放服务端地址信息，将客户端的请求参数数据信息打包成网络消息，再通过网络传输发送给服务端。
- 服务端存根(Server Stub)：接收客户端发送过来的请求消息并进行解包，然后再调用本地服务进行处理。
- 服务端(Server)：服务的真正提供者。
- Network Service：底层传输，可以是 TCP 或 HTTP。

### **5.2.3 Python 自带 RPC Demo**

Server.py：

```python
from SimpleXMLRPCServer import SimpleXMLRPCServer    
def fun_add(a,b): 
    totle = a + b  
    return totle 
if __name__ == '__main__': 
    s = SimpleXMLRPCServer(('0.0.0.0', 8080))   #开启xmlrpcserver 
    s.register_function(fun_add)                #注册函数fun_add 
    print "server is online..." 
    s.serve_forever()                           #开启循环等待 
```

Client.py：

```python
from xmlrpclib import ServerProxy            #导入xmlrpclib的包 
s = ServerProxy("http://172.171.5.205:8080") #定义xmlrpc客户端 
print s.fun_add(2,3)
```

开启服务端：

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\1.jpg)

开启客户端：

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\2.jpg)

### **5.2.4 Wireshark 抓包分析过程**

客户端去往服务端：

- 客户端 IP：172.171.4.176
- 服务端 IP：172.171.5.95

通信使用 HTTP 协议，XML 文件传输格式。传输的字段包括：方法名 methodName，两个参数 2，3。

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\Request 抓包.jpg" style="zoom:50%;" />

在这两次网络传输中使用了 HTTP 协议，建立 HTTP 协议之间有 TCP 三次握手，断开 HTTP 协议时有 TCP 四次挥手。

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\基于 HTTP 协议的 RPC 连接过程.jpg)

### 5.2.5 RPC 调用详细流程图

<img src="C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\RPC 调用详细流程图.jpg" style="zoom:50%;" />

一次 RPC 调用流程如下：

- 服务消费者(Client 客户端)通过本地调用的方式调用服务。
- 客户端存根(Client Stub)接收到调用请求后负责将方法、入参等信息序列化(组装)成能够进行网络传输的消息体。
- 客户端存根(Client Stub)找到远程的服务地址，并且将消息通过网络发送给服务端。
- 服务端存根(Server Stub)收到消息后进行解码(反序列化操作)。
- 服务端存根(Server Stub)根据解码结果调用本地的服务进行相关处理
- 服务端(Server)本地服务业务处理。
- 处理结果返回给服务端存根(Server Stub)。
- 服务端存根(Server Stub)序列化结果。
- 服务端存根(Server Stub)将结果通过网络发送至消费方。
- 客户端存根(Client Stub)接收到消息，并进行解码(反序列化)。
- 服务消费方得到最终结果。

## **5.3 RPC 核心之功能实现**

RPC 的核心功能主要由 5 个模块组成，如果想要自己实现一个 RPC，最简单的方式要实现三个技术点，分别是：

- 服务寻址
- 数据流的序列化和反序列化
- 网络传输

### **5.3.1 服务寻址**

服务寻址可以使用 Call ID 映射。在本地调用中，函数体是直接通过函数指针来指定的，但是在远程调用中，函数指针是不行的，因为两个进程的地址空间是完全不一样的。

所以在 RPC 中，所有的函数都必须有自己的一个 ID。这个 ID 在所有进程中都是唯一确定的。

客户端在做远程过程调用时，必须附上这个 ID。然后我们还需要在客户端和服务端分别维护一个函数和Call ID的对应表。

当客户端需要进行远程调用时，它就查一下这个表，找出相应的 Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。

#### 实现方式：服务注册中心

要调用服务，首先你需要一个服务注册中心去查询对方服务都有哪些实例。**Dubbo 的服务注册中心是可以配置的，官方推荐使用 Zookeeper。**

实现案例：RMI(Remote Method Invocation，远程方法调用)也就是 RPC 本身的实现方式。

![](C:\Users\felixsfan\Desktop\办公机备份\学习\分布式\images\RMI.jpg)

Registry(服务发现)：借助 JNDI 发布并调用了 RMI 服务。实际上，JNDI 就是一个注册表，服务端将服务对象放入到注册表中，客户端从注册表中获取服务对象。

RMI 服务在服务端实现之后需要注册到 RMI Server 上，然后客户端从指定的 RMI 地址上 Lookup 服务，调用该服务对应的方法即可完成远程方法调用。

Registry 是个很重要的功能，当服务端开发完服务之后，要对外暴露，如果没有服务注册，则客户端是无从调用的，即使服务端的服务就在那里。

### **5.3.2 序列化和反序列化**

客户端怎么把参数值传给远程的函数呢?在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。

但是在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数。

这时候就需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。

只有二进制数据才能在网络中传输，**序列化和反序列化的定义**是：

- 将对象转换成二进制流的过程叫做序列化
- 将二进制流转换成对象的过程叫做反序列化

这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。

### **5.3.3 网络传输**

网络传输：远程调用往往用在网络上，客户端和服务端是通过网络连接的。

所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把 Call ID 和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。

尽管大部分 RPC 框架都使用 TCP 协议，但其实 UDP 也可以，而 gRPC 干脆就用了 HTTP2。

TCP 的连接是最常见的，简要分析基于 TCP 的连接：通常 TCP 连接可以是按需连接(需要调用的时候就先建立连接，调用结束后就立马断掉)，也可以是长连接(客户端和服务器建立起连接之后保持长期持有，不管此时有无数据包的发送，可以配合心跳检测机制定期检测建立的连接是否存活有效)，多个远程过程调用共享同一个连接。

所以，要实现一个 RPC 框架，只需要把以下三点实现了就基本完成了：

- Call ID 映射：可以直接使用函数字符串，也可以使用整数 ID。映射表一般就是一个哈希表。
- 序列化反序列化：可以自己写，也可以使用 Protobuf 或者 FlatBuffers 之类的。
- 网络传输库：可以自己写 Socket，或者用 Asio，ZeroMQ，Netty 之类。

### **5.3.4 RPC 核心之网络传输协议**

在第三节中说明了要实现一个 RPC，需要选择网络传输的方式。

在 RPC 中可选的网络传输方式有多种，可以选择 TCP 协议、UDP 协议、HTTP 协议。

每一种协议对整体的性能和效率都有不同的影响，如何选择一个正确的网络传输协议呢?首先要搞明白各种传输协议在 RPC 中的工作方式。

#### **5.3.4.1 基于 TCP 协议的 RPC 调用**

由服务的调用方与服务的提供方建立 Socket 连接，并由服务的调用方通过 Socket 将需要调用的接口名称、方法名称和参数序列化后传递给服务的提供方，服务的提供方反序列化后再利用反射调用相关的方法。

将结果返回给服务的调用方，整个基于 TCP 协议的 RPC 调用大致如此。

但是在实例应用中则会进行一系列的封装，如 RMI 便是在 TCP 协议上传递可序列化的 Java 对象。

#### **5.3.4.2 基于 HTTP 协议的 RPC 调用**

该方法更像是访问网页一样，只是它的返回结果更加单一简单。

而调用的具体方法则是根据 URL 进行方法调用，而方法所需要的参数可能是对服务调用方传输过去的 XML 数据或者 JSON 数据解析后的结果，返回 JOSN 或者 XML 的数据结果。

由于目前有很多开源的 Web 服务器，如 Tomcat，所以其实现起来更加容易，就像做 Web 项目一样。

#### **5.3.4.3 两种方式对比**

基于 TCP 的协议实现的 RPC 调用，由于 TCP 协议处于协议栈的下层，能够更加灵活地对协议字段进行定制，减少网络开销，提高性能，实现更大的吞吐量和并发数。

但是需要更多关注底层复杂的细节，实现的代价更高。同时对不同平台，如安卓，iOS 等，需要重新开发出不同的工具包来进行请求发送和相应解析，工作量大，难以快速响应和满足用户需求。

基于 HTTP 协议实现的 RPC 则可以使用 JSON 和 XML 格式的请求或响应数据。

而 JSON 和 XML 作为通用的格式标准(使用 HTTP 协议也需要序列化和反序列化，不过这不是该协议下关心的内容，成熟的 Web 程序已经做好了序列化内容)，开源的解析工具已经相当成熟，在其上进行二次开发会非常便捷和简单。

但是由于 HTTP 协议是上层协议，发送包含同等内容的信息，使用 HTTP 协议传输所占用的字节数会比使用 TCP 协议传输所占用的字节数更高。

因此在同等网络下，通过 HTTP 协议传输相同内容，效率会比基于 TCP 协议的数据效率要低，信息传输所占用的时间也会更长，当然压缩数据，能够缩小这一差距。

#### **5.3.4.4 使用 RabbitMQ 的 RPC 架构**

在 OpenStack 中服务与服务之间使用 RESTful API 调用，而在服务内部则使用 RPC 调用各个功能模块。

正是由于使用了 RPC 来解耦服务内部功能模块，使得 OpenStack 的服务拥有扩展性强，耦合性低等优点。

OpenStack 的 RPC 架构中，加入了消息队列 RabbitMQ，这样做的目的是为了保证 RPC 在消息传递过程中的安全性和稳定性。

### **5.4.5 简单对比 RPC 和 Restful API**

#### **5.4.5.1 RESTful API 架构**

REST 的几个特点为：资源、统一接口、URI 和无状态。

**①资源**

所谓"资源"，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，就是一个具体的实在。

**②统一接口**

RESTful 架构风格规定，数据的元操作，即 CRUD(Create，Read，Update 和 Delete，即数据的增删查改)操作，分别对应于 HTTP 方法：GET 用来获取资源，POST 用来新建资源(也可以用于更新资源)，PUT 用来更新资源，DELETE 用来删除资源，这样就统一了数据操作的接口，仅通过 HTTP 方法，就可以完成对数据的所有增删查改工作。

**③URL**

可以用一个 URI(统一资源定位符)指向资源，即每个 URI 都对应一个特定的资源。

要获取这个资源，访问它的 URI 就可以，因此 URI 就成了每一个资源的地址或识别符。

**④无状态**

所谓无状态的，即所有的资源，都可以通过 URI 定位，而且这个定位与其他资源无关，也不会因为其他资源的变化而改变。有状态和无状态的区别，举个简单的例子说明一下。

如查询员工的工资，如果查询工资是需要登录系统，进入查询工资的页面，执行相关操作后，获取工资的多少，则这种情况是有状态的。

因为查询工资的每一步操作都依赖于前一步操作，只要前置操作不成功，后续操作就无法执行。

如果输入一个 URI 即可得到指定员工的工资，则这种情况是无状态的，因为获取工资不依赖于其他资源或状态。

且这种情况下，员工工资是一个资源，由一个 URI 与之对应，可以通过 HTTP 中的 GET 方法得到资源，这是典型的 RESTful 风格。

#### **5.4.5.2 总结**

RPC 主要用于公司内部的服务调用，性能消耗低，传输效率高，实现复杂。

HTTP 主要用于对外的异构环境，浏览器接口调用，App 接口调用，第三方接口调用等。

RPC 使用场景(大型的网站，内部子系统较多、接口非常多的情况下适合使用 RPC)：

- 长链接。不必每次通信都要像 HTTP 一样去 3 次握手，减少了网络开销。
- 注册发布机制。RPC 框架一般都有注册中心，有丰富的监控管理;发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。
- 安全性，没有暴露资源操作。
- 微服务支持。就是最近流行的服务化架构、服务化治理，RPC 框架是一个强力的支撑。

## 5.2 dubbo

### Dubbo内置负载均衡策略

Dubbo内置了4种负载均衡策略:

1. RandomLoadBalance:随机负载均衡。随机的选择一个。是Dubbo的**默认**负载均衡策略。
2. RoundRobinLoadBalance:轮询负载均衡。轮询选择一个。
3. LeastActiveLoadBalance:最少活跃调用数，相同活跃数的随机。活跃数指调用前后计数差。使慢的 Provider 收到更少请求，因为越慢的 Provider 的调用前后计数差会越大。
4. ConsistentHashLoadBalance:一致性哈希负载均衡。相同参数的请求总是落在同一台机器上。

### 1.随机负载均衡

顾名思义，随机负载均衡策略就是从多个 Provider 中随机选择一个。但是 Dubbo 中的随机负载均衡有一个权重的概念，即按照权重设置随机概率。比如说，有10个 Provider，并不是说，每个 Provider 的概率都是一样的，而是要结合这10个 Provider 的权重来分配概率。

Dubbo中，可以对 Provider 设置权重。比如机器性能好的，可以设置大一点的权重，性能差的，可以设置小一点的权重。权重会对负载均衡产生影响。可以在Dubbo Admin中对 Provider 进行权重的设置。

**基于权重的负载均衡算法**

随机策略会先判断所有的 Invoker 的权重是不是一样的，如果都是一样的，那么处理就比较简单了。使用random.nexInt(length)就可以随机生成一个 Invoker 的序号,根据序号选择对应的 Invoker 。如果没有在Dubbo Admin中对服务 Provider 设置权重，那么所有的 Invoker 的权重就是一样的，默认是100。 如果权重不一样，那就需要结合权重来设置随机概率了。算法大概如下： 假如有4个 Invoker。

| invoker | weight |
| ------- | ------ |
| A       | 10     |
| B       | 20     |
| C       | 20     |
| D       | 30     |

A，B，C和D总的权重是10 + 20 + 20 + 30 = 80。将80个数分布在如下的图中:

```
+-----------------------------------------------------------------------------------+
|          |                    |                    |                              |
+-----------------------------------------------------------------------------------+
1          10                   30                   50                             80

|-----A----|---------B----------|----------C---------|---------------D--------------|


---------------------15

-------------------------------------------37

-----------------------------------------------------------54
```

上面的图中一共有4块区域，长度分别是A，B，C和D的权重。使用random.nextInt(10 + 20 + 20 + 30)，从80个数中随机选择一个。然后再判断该数分布在哪个区域。比如，如果随机到37，37是分布在C区域的，那么就选择 Invoker C。15是在B区域，54是在D区域。

**随机负载均衡源码**

下面是随机负载均衡的源码，为了方便阅读和理解，我把无关部分都去掉了。

```
public class RandomLoadBalance extends AbstractLoadBalance {

    private final Random random = new Random();

    protected <T> Invoker<T> doSelect(List<Invoker<T>> invokers, URL url, Invocation invocation) {
        int length = invokers.size();      // Invoker 总数
        int totalWeight = 0;               // 所有 Invoker 的权重的和

        // 判断是不是所有的 Invoker 的权重都是一样的
        // 如果权重都一样，就简单了。直接用Random生成索引就可以了。
        boolean sameWeight = true;
        for (int i = 0; i < length; i++) {
            int weight = getWeight(invokers.get(i), invocation);
            totalWeight += weight; // Sum
            if (sameWeight && i > 0 && weight != getWeight(invokers.get(i - 1), invocation)) {
                sameWeight = false;
            }
        }

        if (totalWeight > 0 && !sameWeight) {
            // 如果不是所有的 Invoker 权重都相同，那么基于权重来随机选择。权重越大的，被选中的概率越大
            int offset = random.nextInt(totalWeight);
            for (int i = 0; i < length; i++) {
                offset -= getWeight(invokers.get(i), invocation);
                if (offset < 0) {
                    return invokers.get(i);
                }
            }
        }
        // 如果所有 Invoker 权重相同
        return invokers.get(random.nextInt(length));
    }
}
```

### 2.轮询负载均衡

轮询负载均衡，就是依次的调用所有的 Provider。和随机负载均衡策略一样，轮询负载均衡策略也有权重的概念。 轮询负载均衡算法可以让RPC调用严格按照我们设置的比例来分配。不管是少量的调用还是大量的调用。但是轮询负载均衡算法也有不足的地方，存在慢的 Provider 累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上，导致整个系统变慢。

### 3.最少活跃调用数负载均衡

官方解释：

> 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差，使慢的机器收到更少。

这个解释好像说的不是太明白。目的是让更慢的机器收到更少的请求，但具体怎么实现的还是不太清楚。举个例子：每个服务维护一个活跃数计数器。当A机器开始处理请求，该计数器加1，此时A还未处理完成。若处理完毕则计数器减1。而B机器接受到请求后很快处理完毕。那么A,B的活跃数分别是1，0。当又产生了一个新的请求，则选择B机器去执行(B活跃数最小)，这样使慢的机器A收到少的请求。

处理一个新的请求时，Consumer 会检查所有 Provider 的活跃数，如果具有最小活跃数的 Invoker 只有一个，直接返回该 Invoker：

```java
if (leastCount == 1) {
    // 如果只有一个最小则直接返回
    return invokers.get(leastIndexs[0]);
}
```

如果最小活跃数的 Invoker 有多个，且权重不相等同时总权重大于0，这时随机生成一个权重，范围在 (0，totalWeight) 间内。最后根据随机生成的权重，来选择 Invoker。

```
if (! sameWeight && totalWeight > 0) {
    // 如果权重不相同且权重大于0则按总权重数随机
    int offsetWeight = random.nextInt(totalWeight);
    // 并确定随机值落在哪个片断上
    for (int i = 0; i < leastCount; i++) {
        int leastIndex = leastIndexs[i];
        offsetWeight -= getWeight(invokers.get(leastIndex), invocation);
        if (offsetWeight <= 0)
            return invokers.get(leastIndex);
    }
}
```

### 4.一致性Hash算法

使用一致性 Hash 算法，让相同参数的请求总是发到同一 Provider。 当某一台 Provider 崩溃时，原本发往该 Provider 的请求，基于虚拟节点，平摊到其它 Provider，不会引起剧烈变动。 算法参见：<http://en.wikipedia.org/wiki/Consistent_hashing>。

缺省只对第一个参数Hash，如果要修改，请配置:

```
<dubbo:parameter key="hash.arguments" value="0,1" />
```

缺省用160份虚拟节点，如果要修改，请配置:

```
<dubbo:parameter key="hash.nodes" value="320" />
```

一致性Hash算法可以和缓存机制配合起来使用。比如有一个服务getUserInfo(String userId)。设置了Hash算法后，相同的userId的调用，都会发送到同一个 Provider。这个 Provider 上可以把用户数据在内存中进行缓存，减少访问数据库或分布式缓存的次数。如果业务上允许这部分数据有一段时间的不一致，可以考虑这种做法。减少对数据库，缓存等中间件的依赖和访问次数，同时减少了网络IO操作，提高系统性能。

## 5.3 Zookeeper

见Zookeeper文件

Zookeeper本身是不提供负载均衡的策略，需要自己来实现（由Dubbo实现），所以这里确切的说，是在负载均衡中应用到了Zookeeper集群来做服务注册和协调

## 5.4 负载均衡

### 5.4.1 实现方式

参考数据库文件夹分布式数据库负载均衡解决方案

在决定如何实现负载均衡时，应该考虑到这些因素。有许多负载均衡解决方案可以使用，从诸如 Wackamole（http∶/wwwbackhand.org/wackamole/）这样基于端点的（peer-based）实现，到 DNS、LVS（Linux Virtual Server，http∶/www.linuvirtualserver.org）、硬件负载均衡器、TCP 代理、MySQLProxy，以及在应用中管理负载均衡。

### 5.4.2 负载均衡算法

有许多算法用来决定哪个服务器接受下一个连接。每个厂商都有各自不同的算法，下面这个清单列出了一些可用的方法∶ 

**随机**

负载均衡器随机地从可用的服务器池中选择一个服务器来处理请求。

**轮询**

负载均衡器以循环顺序发送请求到服务器，例如∶A，B，C，A，B，C。

**最少连接数**

下一个连接请求分配给拥有最少活跃连接的服务器。

**最快响应**

能够最快处理请求的服务器接受下一个连接。当服务器池里同时存在快速和慢速服务器时，这很有效。即使同样的查询在不同的场景下运行也会有不同的表现，例如当查询结果已经缓存在查询缓存中，或者服务器缓存中已经包含了所需要的数据时。

**哈希（一致性哈希）**

负载均衡器通过连接的源IP 地址进行哈希，将其映射到池中的同一个服务器上。每次从同一个IP 地址发起请求，负载均衡器都会将请求发送给同样的服务器。只有当池中服务器数目改变时这种绑定才会发生变化。

**权重**

负载均衡器能够结合使用上述几种算法。例如，你可能拥有单CPU和双CPU的机器。双CPU机器有接近两倍的性能，所以可以让负载均衡器分派两倍的请求给双CPU机器。

我们这里只描述了即时处理请求的算法，无须对连接请求排队。但有时候使用排队算法可能更有效。例如，一个算法可能只维护给定的数据库服务器并发数目，同一时刻只允许不超过 N个活跃事务。如果有太多的活跃事务，就将新的请求放到一个队列里，然后让可用服务器列表的第一个来处理它。有些连接池也支持队列算法。

# 6.分布式锁

## 6.1 redis实现



## 6.2 Zookeeper实现



## 6.3 数据库实现



# 7.分布式定时任务

quartz

时间戳同步

# 8.分布式搜索引擎



# 9.分布式会话

单点登录

# 10.分布式链路追踪



# 11.高并发架构

参考后台高性能文件夹